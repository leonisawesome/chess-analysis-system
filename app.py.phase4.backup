#!/usr/bin/env python3
"""
System A Web UI
Flask web interface for chess knowledge retrieval with interactive diagrams
"""

import os
import re
import time
import json
import chess
import chess.pgn
import chess.svg
import spacy
from io import StringIO
from flask import Flask, render_template, request, jsonify
from openai import OpenAI
from qdrant_client import QdrantClient
from query_system_a import query_system_a, COLLECTION_NAME, QDRANT_PATH, embed_query, semantic_search, gpt5_rerank, TOP_K, TOP_N
from opening_data import detect_opening
import fen_validator
import query_classifier

from chess_positions import detect_fen, parse_moves_to_fen, extract_chess_positions, filter_relevant_positions, create_lichess_url
from diagram_processor import extract_moves_from_description, extract_diagram_markers, replace_markers_with_ids
from opening_validator import extract_contamination_details, generate_section_with_retry, validate_stage2_diagrams, validate_and_fix_diagrams

# Feature flag for dynamic middlegame pipeline
USE_DYNAMIC_PIPELINE = True  # Set to False to disable middlegame handling

app = Flask(__name__)
app.config['TEMPLATES_AUTO_RELOAD'] = True

# Initialize clients at module level (created ONCE on startup)
print("Initializing clients...")
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable not set!")

OPENAI_CLIENT = OpenAI(api_key=api_key)
QDRANT_CLIENT = QdrantClient(path=QDRANT_PATH)
print(f"‚úì Clients initialized (Qdrant: {QDRANT_CLIENT.count(COLLECTION_NAME).count} vectors)")

# Load spaCy model for smart caption extraction
print("Loading spaCy model...")
try:
    NLP = spacy.load("en_core_web_sm")
    print("‚úì spaCy model loaded")
except OSError:
    print("‚ö†Ô∏è  spaCy model not found. Run: python -m spacy download en_core_web_sm")
    NLP = None

# Load canonical FENs for middlegame concepts
print("Loading canonical FENs...")
CANONICAL_FENS = {}
try:
    with open('canonical_fens.json', 'r') as f:
        CANONICAL_FENS = json.load(f)
    print(f"‚úì Loaded {len(CANONICAL_FENS)} canonical FEN concepts")
except FileNotFoundError:
    print("‚ö†Ô∏è  canonical_fens.json not found - middlegame queries will use RAG only")


# ============================================================================
# MULTI-STAGE SYNTHESIS PIPELINE
# ============================================================================

import json

def stage1_generate_outline(openai_client, query: str, top_chunks: list) -> dict:
    """
    Stage 1: Generate structured outline from query and top chunks.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        top_chunks: List of top 5 relevant text chunks

    Returns:
        dict with sections: {'sections': [{'title': ..., 'bullets': [...]}]}
    """
    # Combine top chunks into context
    context = "\n\n---\n\n".join([chunk[:800] for chunk in top_chunks[:5]])

    prompt = f"""You are a chess instructor creating a comprehensive guide. Based on the user's question and reference material, create a structured outline.

User Question: {query}

Reference Material:
{context}

Create a JSON outline with 5-7 sections. Each section should have:
- title: A clear section heading (e.g., "Overview", "Strategic Themes", "Main Variations", "Key Plans", "Common Tactics")
- bullets: 3-5 bullet points summarizing key information for that section
- diagram_anchor: A standard diagram position for this section (see templates below)

DIAGRAM ANCHOR TEMPLATES:
When creating the outline, specify which standard diagram position to use for each major section:

For Sicilian Defense queries, use these anchors:
- Introduction section: [ANCHOR: Basic Sicilian - 1.e4 c5]
- Najdorf section: [ANCHOR: Najdorf - 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6]
- Dragon section: [ANCHOR: Dragon - 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 g6]
- Sveshnikov section: [ANCHOR: Sveshnikov - 1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 e5]
- Strategic themes section: [ANCHOR: Typical Sicilian pawn structure]

For each section in your JSON outline, include a "diagram_anchor" field with the appropriate anchor.

Return ONLY valid JSON in this exact format:
{{
  "sections": [
    {{"title": "Overview", "bullets": ["...", "...", "..."], "diagram_anchor": "[ANCHOR: Basic Sicilian - 1.e4 c5]"}},
    {{"title": "Strategic Themes", "bullets": ["...", "..."], "diagram_anchor": "[ANCHOR: Typical Sicilian pawn structure]"}},
    ...
  ]
}}"""

    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_completion_tokens=1500
    )

    try:
        raw_response = response.choices[0].message.content

        # Strip markdown code blocks if present
        if "```json" in raw_response:
            raw_response = raw_response.split("```json")[1].split("```")[0]
        elif "```" in raw_response:
            raw_response = raw_response.split("```")[1].split("```")[0]

        outline = json.loads(raw_response.strip())
        print(f"[Stage 1] ‚úÖ Generated outline with {len(outline['sections'])} sections")
        return outline

    except json.JSONDecodeError as e:
        print(f"[Stage 1] ‚ùå JSON parse error: {e}")
        print(f"[Stage 1] Raw response: {response.choices[0].message.content[:500]}...")
        return {"sections": [{"title": "Overview", "bullets": ["General information about " + query]}]}
    except Exception as e:
        print(f"[Stage 1] ‚ùå Unexpected error: {e}")
        print(f"[Stage 1] Raw response: {response.choices[0].message.content[:500]}...")
        return {"sections": [{"title": "Overview", "bullets": ["General information about " + query]}]}
def stage2_expand_sections(openai_client, query: str, sections: list, context: str) -> list:
    """
    Stage 2: Expand each section into 150-300 words with chess notation and diagram markers.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        sections: List of section dicts from stage 1
        context: Combined reference text

    Returns:
        List of expanded section texts
    """
    # ITEM-008: Detect opening BEFORE loop to use in retry logic
    detected_opening, expected_signature, eco_code = detect_opening(query)

    if detected_opening:
        print(f"\n[ITEM-008] Detected opening: {detected_opening}")
        print(f"[ITEM-008] Expected signature: {expected_signature}")
        print(f"[ITEM-008] Regeneration feedback loop ENABLED")
    else:
        print(f"\n[ITEM-008] No opening detected - regeneration feedback loop DISABLED")

    expanded_sections = []

    # ITEM-008: Use regeneration feedback loop for each section
    for section in sections:
        title = section['title']

        if detected_opening:
            # Use retry logic with contamination detection
            print(f"\n[ITEM-008] Processing section '{title}' with retry logic")
            expanded_content, success, attempts = generate_section_with_retry(
                openai_client, section, query, context,
                detected_opening, expected_signature
            )

            if success:
                print(f"[ITEM-008] ‚úÖ Section '{title}' generated successfully after {attempts} attempt(s)")
            else:
                print(f"[ITEM-008] ‚ö†Ô∏è Section '{title}' still contaminated after {attempts} attempts")

            expanded_sections.append(f"## {title}\n\n{expanded_content}")
            print(f"[Stage 2] Expanded section: {title} ({len(expanded_content.split())} words)")
        else:
            # No opening detected - use simplified generation without retry
            bullets = section['bullets']
            bullets_text = "\n".join([f"- {b}" for b in bullets])

            prompt = f"""You are writing a section of a chess guide. Expand the following outline into 150-300 words.

Section Title: {title}
Key Points:
{bullets_text}

Reference Context (if needed):
{context[:1500]}

Requirements:
- Write 150-300 words in an educational, engaging style
- Include specific chess moves in algebraic notation where appropriate
- Be specific and practical
- Focus on understanding, not just memorization

Write the expanded section now:"""

            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are an expert chess instructor creating educational content."},
                    {"role": "user", "content": prompt}
                ],
                max_completion_tokens=800
            )

            expanded = response.choices[0].message.content.strip()
            expanded_sections.append(f"## {title}\n\n{expanded}")
            print(f"[Stage 2] Expanded section: {title} ({len(expanded.split())} words)")

    # ITEM-008: Validation now happens during generation inside generate_section_with_retry()
    # Old post-loop validation removed since it's redundant
    return expanded_sections
def stage3_final_assembly(openai_client, query: str, expanded_sections: list) -> str:
    """
    Stage 3: Assemble expanded sections into coherent 800-1500 word article.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        expanded_sections: List of expanded section texts

    Returns:
        Final synthesized article text
    """
    sections_text = "\n\n".join(expanded_sections)

    # Count diagrams BEFORE Stage 3
    diagram_count = sections_text.count('[DIAGRAM:')
    print(f"[Stage 3] Input has {diagram_count} diagram markers")

    prompt = f"""You are combining multiple draft sections into a cohesive chess article.

CRITICAL STRUCTURE INTEGRITY REQUIREMENT:
- Input contains {diagram_count} diagram markers in the format [DIAGRAM: ...].
- Every diagram marker MUST appear in the final article exactly as written.
- These markers are structural and cannot be edited, merged, or deleted.
- You may move them for readability but not remove them.
- Output MUST contain exactly {diagram_count} markers.
- If any are missing, your answer is INVALID.

User Question: {query}

Input sections:
{sections_text}

Requirements:
- Add smooth transitions between sections
- Ensure logical flow from basics to advanced concepts
- Add a brief "Study Recommendations" section at the end
- Target length: 800-1500 words
- Preserve all {diagram_count} diagram markers exactly

Now produce the final integrated article:"""

    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_completion_tokens=2500
    )

    article = response.choices[0].message.content.strip()

    # POST-VALIDATION: Check diagram count
    output_diagram_count = article.count('[DIAGRAM:')
    print(f"[Stage 3] Output has {output_diagram_count}/{diagram_count} markers")

    if output_diagram_count < diagram_count:
        print(f"[Stage 3] ‚ö†Ô∏è VALIDATION FAILED: Missing {diagram_count - output_diagram_count} markers")
        print(f"[Stage 3] Attempting corrective retry...")

        correction_prompt = f"""You failed to preserve all diagram markers in your previous attempt.

ORIGINAL STAGE 3 INPUT (which had {diagram_count} markers):
{sections_text}

CRITICAL STRUCTURE INTEGRITY REQUIREMENT:
- Input contains {diagram_count} diagram markers in the format [DIAGRAM: ...].
- Every diagram marker MUST appear in the final article exactly as written.
- Output MUST contain exactly {diagram_count} markers.
- If any are missing, your answer is INVALID.

Your previous output had only {output_diagram_count} markers (INCORRECT).

Requirements:
- Add smooth transitions between sections
- Ensure logical flow from basics to advanced concepts
- Add a brief "Study Recommendations" section at the end
- Target length: 800-1500 words
- Preserve all {diagram_count} diagram markers exactly

Regenerate the complete article with ALL {diagram_count} markers preserved:"""

        retry_response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": correction_prompt}],
            max_completion_tokens=2500
        )

        article = retry_response.choices[0].message.content.strip()
        final_count = article.count('[DIAGRAM:')
        print(f"[Stage 3] Retry result: {final_count}/{diagram_count} markers")

    word_count = len(article.split())
    print(f"[Stage 3] Final article: {word_count} words")

    return article
def synthesize_answer(openai_client, query: str, top_chunks: list) -> str:
    """
    Complete 3-stage synthesis pipeline.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        top_chunks: List of top ranked text chunks

    Returns:
        Synthesized answer text
    """
    print("\n" + "="*60)
    print("MULTI-STAGE SYNTHESIS PIPELINE")
    print("="*60)

    # Stage 1: Generate outline
    outline = stage1_generate_outline(openai_client, query, top_chunks)

    # Stage 2: Expand sections
    context = "\n\n".join([chunk[:1000] for chunk in top_chunks[:8]])
    expanded_sections = stage2_expand_sections(
        openai_client,
        query,
        outline['sections'],
        context
    )

    # Stage 2.5: Validate diagrams and fix if missing
    expanded_sections = validate_and_fix_diagrams(
        openai_client,
        query,
        expanded_sections,
        context
    )

    # Stage 3: Final assembly
    final_article = stage3_final_assembly(openai_client, query, expanded_sections)

    print("="*60 + "\n")

    return final_article




# ============================================================================
# FLASK ROUTES
# ============================================================================

@app.route('/')
def index():
    """Main page."""
    return render_template('index.html')


@app.route('/test', methods=['POST'])
def test():
    """Test endpoint."""
    return jsonify({'status': 'ok', 'message': 'test works'})


@app.route('/query', methods=['POST'])
def query():
    """Handle query requests with detailed timing."""
    print("=" * 80)
    print("QUERY ENDPOINT CALLED")
    print("=" * 80)
    try:
        start = time.time()

        # Step 1: Parse request
        data = request.get_json()
        query_text = data.get('query', '').strip()
        t1 = time.time()
        print(f"‚è±  Request parsing: {t1-start:.2f}s")

        if not query_text:
            return jsonify({'error': 'Query cannot be empty'}), 400

        # Step 1.5: Classify query and get canonical FEN if available
        query_type, concept_key, canonical_fen = query_classifier.get_canonical_fen_for_query(
            query_text,
            CANONICAL_FENS
        )
        print(f"üìã Query type: {query_type}")
        if concept_key:
            print(f"üìã Concept: {concept_key}")
        if canonical_fen:
            print(f"‚úì Using canonical FEN: {canonical_fen}")

        # Step 2: Generate embedding
        query_vector = embed_query(OPENAI_CLIENT, query_text)
        t2 = time.time()
        print(f"‚è±  Embedding: {t2-t1:.2f}s")

        # Step 3: Search Qdrant
        candidates = semantic_search(QDRANT_CLIENT, query_vector, top_k=TOP_K)
        t3 = time.time()
        print(f"‚è±  Qdrant search: {t3-t2:.2f}s")

        # Step 4: Rerank with GPT-5
        ranked_results = gpt5_rerank(OPENAI_CLIENT, query_text, candidates, top_k=TOP_N)
        t4 = time.time()
        print(f"‚è±  GPT-5 reranking: {t4-t3:.2f}s")

        # Step 5: Format results for web display
        results = []
        for candidate, score in ranked_results:
            payload = candidate.payload

            # Extract metadata
            book_name = payload.get('book_name', 'Unknown')
            if book_name.endswith('.epub') or book_name.endswith('.mobi'):
                book_name = book_name[:-5]

            text = payload.get('text', '')
            chapter = payload.get('chapter_title', '')

            # Extract chess positions from text (pass query for relevance filtering)
            positions = extract_chess_positions(text, query=query_text)

            # Format result
            result = {
                'score': round(score, 1),
                'book_name': book_name,
                'book': book_name,  # Keep for backwards compatibility
                'chapter_title': chapter,
                'chapter': chapter,  # Keep for backwards compatibility
                'text': text,
                'positions': positions,
                'has_positions': len(positions) > 0
            }
            results.append(result)

        t5 = time.time()
        print(f"‚è±  Response formatting: {t5-t4:.2f}s")

        # DEBUG: Check position extraction
        print("\n=== POSITION EXTRACTION DEBUG ===")
        for i, result in enumerate(results[:5]):
            chunk_text = result.get('text', '')
            print(f"\nSource {i+1}:")
            print(f"  Book: {result.get('book_name', 'unknown')}")
            print(f"  Text preview: {chunk_text[:100]}...")

            # Check what position data exists
            if 'positions' in result:
                print(f"  ‚úÖ Positions array length: {len(result['positions'])}")
                if len(result['positions']) > 0:
                    print(f"     First position: {result['positions'][0]}")
            else:
                print(f"  ‚ùå No 'positions' key")

            if 'has_positions' in result:
                print(f"  has_positions flag: {result['has_positions']}")
            else:
                print(f"  ‚ùå No 'has_positions' key")

            # Try to detect patterns in text
            import re
            fen_pattern = r'[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}'
            moves_pattern = r'1\.\s*e4\s+c5'

            if re.search(fen_pattern, chunk_text):
                print(f"  üìã FEN detected in text")
            if re.search(moves_pattern, chunk_text, re.IGNORECASE):
                print(f"  ‚ôü Moves '1.e4 c5' detected in text")
        print("=== END POSITION DEBUG ===\n")

        # Step 6: Synthesize coherent answer using 3-stage pipeline
        print(f"‚è±  Starting 3-stage synthesis pipeline...")
        synthesis_start = time.time()

        # Prepare context with canonical FEN if available
        context_chunks = [r['text'] for r in results[:8]]
        if canonical_fen:
            context_chunks.insert(0, f"[CANONICAL POSITION: {canonical_fen}]")
            print(f"üìã Injected canonical FEN into synthesis context")

        # Call the new 3-stage synthesis function
        synthesized_answer = synthesize_answer(
            OPENAI_CLIENT,
            query_text,
            context_chunks
        )

        t6 = time.time()
        print(f"‚è±  3-stage synthesis complete: {t6-synthesis_start:.2f}s")

        # Step 6.5: Extract and parse diagram markers from synthesized text
        print(f"\n‚è±  Extracting diagram markers...")
        diagram_start = time.time()

        diagram_positions = extract_diagram_markers(synthesized_answer)
        synthesized_answer = replace_markers_with_ids(synthesized_answer, diagram_positions)

        diagram_time = time.time() - diagram_start
        print(f"‚è±  Diagram extraction complete: {diagram_time:.2f}s")
        print(f"üìã Extracted {len(diagram_positions)} diagram positions from synthesis")

        total = time.time() - start
        print(f"üéØ TOTAL: {total:.2f}s")
        print("=" * 80)

        # DEBUG: Check what's actually being sent to frontend
        print("\n" + "="*60)
        print("FINAL RESPONSE TO FRONTEND - POSITION DEBUG")
        print("="*60)

        for i, source in enumerate(results[:5]):
            print(f"\nSource {i+1}:")
            print(f"  Book: {source.get('book_name', 'unknown')[:50]}")
            print(f"  Has 'positions' key: {'positions' in source}")
            print(f"  Has 'has_positions' key: {'has_positions' in source}")

            if 'positions' in source:
                positions = source['positions']
                print(f"  positions value: {positions}")
                print(f"  positions type: {type(positions)}")
                print(f"  positions length: {len(positions) if positions else 0}")

                if positions:
                    print(f"  First position: {positions[0]}")

            if 'has_positions' in source:
                print(f"  has_positions: {source['has_positions']}")

        print("\n" + "="*60)
        print("END POSITION DEBUG")
        print("="*60 + "\n")

        # Collect positions from top sources for answer section
        synthesized_positions = []
        for result in results[:5]:
            if result.get('has_positions') and result.get('positions'):
                for pos in result['positions']:
                    # Avoid duplicates (same FEN)
                    if not any(p['fen'] == pos['fen'] for p in synthesized_positions):
                        synthesized_positions.append(pos)
                        if len(synthesized_positions) >= 2:  # Max 2 boards in answer
                            break
            if len(synthesized_positions) >= 2:
                break

        print(f"üìã Collected {len(synthesized_positions)} positions for answer section")

        # Prepare response
        response_data = {
            'success': True,
            'query': query_text,
            'answer': synthesized_answer,
            'positions': synthesized_positions,  # Positions extracted from source chunks
            'diagram_positions': diagram_positions,  # NEW: Positions from [DIAGRAM: ...] markers in synthesis
            'sources': results[:5],
            'results': results,  # Keep for backwards compatibility
            'timing': {
                'embedding': round(t2 - t1, 2),
                'search': round(t3 - t2, 2),
                'reranking': round(t4 - t3, 2),
                'formatting': round(t5 - t4, 2),
                'synthesis': round(t6 - synthesis_start, 2),
                'diagrams': round(diagram_time, 2),
                'total': round(total, 2)
            }
        }

        # DEBUG: Log final response structure
        print("\n" + "="*80)
        print("=== FINAL RESPONSE STRUCTURE ===")
        print(f"Response keys: {list(response_data.keys())}")
        print(f"Has 'positions' key: {'positions' in response_data}")
        if 'positions' in response_data:
            print(f"Number of positions: {len(response_data['positions'])}")
            if len(response_data['positions']) > 0:
                print(f"First position keys: {list(response_data['positions'][0].keys())}")
                print(f"First position FEN: {response_data['positions'][0].get('fen', 'N/A')}")
                print(f"First position has SVG: {bool(response_data['positions'][0].get('svg'))}")
        print("="*80 + "\n")

        return jsonify(response_data)

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"ERROR IN /query ENDPOINT:")
        print(error_details)
        return jsonify({'error': str(e), 'details': error_details}), 500


@app.route('/fen_to_lichess', methods=['POST'])
def fen_to_lichess():
    """Convert FEN to Lichess URL."""
    try:
        data = request.get_json()
        fen = data.get('fen', '')

        if not fen:
            return jsonify({'error': 'FEN cannot be empty'}), 400

        url = create_lichess_url(fen)
        return jsonify({'url': url})

    except Exception as e:
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    # Check for API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ùå Error: OPENAI_API_KEY environment variable not set!")
        print("   Set it with: export OPENAI_API_KEY='your-key-here'")
        exit(1)

    print("=" * 80)
    print("SYSTEM A WEB UI")
    print("=" * 80)
    print(f"Corpus: 357,957 chunks from 1,052 books")
    print(f"Starting server at http://127.0.0.1:5001")
    print("=" * 80)

    app.run(debug=False, host='0.0.0.0', port=5001)

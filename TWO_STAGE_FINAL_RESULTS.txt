================================================================================
TWO-STAGE RERANKING: FINAL RESULTS
================================================================================

**Test Date**: 2025-10-26
**Configuration**: Two-stage reranking with intent-aware boosting
**Goal**: Achieve ≥88% precision@5 (vs 86-87% baseline)

================================================================================
OVERALL RESULTS
================================================================================

**Overall Precision@5: 82.0%**

**vs Baseline (86-87%): -4 to -5pp regression** ❌

**Comparison to Previous Optimization Attempts:**
- Baseline (Approaches 1+2): 86-87%
- Unconstrained expansion: 80% (-6 to -7pp)
- Constrained (0.75): 80% (-6 to -7pp)
- Dual-Query Fusion: 80% (-6 to -7pp)
- **Two-Stage Reranking: 82% (-4 to -5pp)**

**Assessment**: Two-stage reranking is slightly better than previous optimization attempts (+2pp) but still falls below baseline by 4-5pp and does NOT meet the ≥88% target.

================================================================================
PER-QUERY BREAKDOWN
================================================================================

Query 1: How do I improve my calculation in the middlegame?
Manual Evaluation: [y, y, p, p, p] = 3.5/5
**Precision: 70%**

Results:
1. [9.0/10] Actionable advice on when to stop calculating → y
2. [8.5/10] How to Calculate Chess Tactics - thinking methods → y
3. [7.5/10] Evaluation's role in comparing lines → p
4. [7.5/10] Evaluative factors that guide calculation → p
5. [7.0/10] Practical expectations about calculation depth → p

Issues: Results 3-5 are conceptual/contextual, not direct improvement methods

---

Query 2: What are the main ideas in the French Defense?
Manual Evaluation: [y, y, p, p, p] = 3.5/5
**Precision: 70%**

Results:
1. [9.0/10] Core French themes - target-free structure, bad bishop → y
2. [8.0/10] Strategic identity and nature of positions → y
3. [7.0/10] Exchange Variation specific ideas → p
4. [6.5/10] Winawer - descriptive rather than outlining plans → p
5. [6.5/10] Offbeat anti-French systems → p

Issues: Results 3-5 are variation-specific or descriptive, not main ideas

---

Query 3: When should I trade pieces in the endgame?
Manual Evaluation: [y, y, y, p, p] = 4.0/5
**Precision: 80%**

Results:
1. [9.0/10] Principles of trading pieces and when to exchange → y
2. [9.0/10] Which endgames to aim for - guidance on trading → y
3. [8.5/10] Concrete queen trade example → y
4. [8.2/10] Endgame strategy - "likely including" simplification → p
5. [7.0/10] Partial guidance on trading in won endgames → p

---

Query 4: How do I create weaknesses in my opponent's position?
Manual Evaluation: [y, y, y, y, y] = 5.0/5
**Precision: 100%** ✓✓✓

Results:
1. [8.5/10] How advancing pawns weakens castle → y
2. [8.5/10] Restraining and fixing doubled pawns → y
3. [8.0/10] Classic Bxh3 break to tear open castle → y
4. [7.5/10] Undermining pawn break concept → y
5. [7.0/10] Preparing pawn breaks like f4 → y

**Perfect results** - all 5 results directly answer the query

---

Query 5: What is the best way to study chess openings? ⭐ CRITICAL
Manual Evaluation: [y, y, y, p, p] = 4.0/5
**Precision: 80%**

**vs 90% baseline = -10pp regression** ❌

Results:
1. [9.3/10] How to Build Your Chess Opening Repertoire → y
2. [8.4/10] Amount of study needed - maintenance and planning → y ⭐
3. [7.8/10] Understanding ideas and interconnections → y
4. [7.2/10] FCO - comprehensive opening guide (resource) → p
5. [7.1/10] Cunning Chess Opening Repertoire (specific book) → p

Issues:
- Results 4-5 are book recommendations, not general study methods
- Still -10pp below baseline despite intent boosting
- Better than unconstrained expansion (60%) but not good enough

---

Query 6: How do I defend against aggressive attacks?
Manual Evaluation: [y, y, y, p, p] = 4.0/5
**Precision: 80%**

Results:
1. [9.0/10] Clear options for meeting attacks (move, capture, block) → y
2. [8.5/10] Fortress and blockade defensive methods → y
3. [8.0/10] Core defensive skills and counter-attack → y
4. [7.5/10] Back-rank vulnerability (endgame-specific) → p
5. [6.5/10] Defensive drawing technique (too endgame-specific) → p

Issues: Results 4-5 are endgame-specific, not general defense

---

Query 7: What endgame principles should beginners learn first?
Manual Evaluation: [y, y, y, y, p] = 4.5/5
**Precision: 90%**

Results:
1. [9.5/10] First endgame tasks - learning simple mates → y
2. [8.5/10] Rook endings most common and important → y
3. [7.5/10] Piece value and winning chances in pawn endings → y
4. [7.5/10] Minimal core curriculum - actionable prioritization → y
5. [6.5/10] Classical K+P ending example (not principles) → p

Good results - 4 strong principles, 1 example

---

Query 8: How do I improve my positional understanding?
Manual Evaluation: [y, y, p, p, p] = 3.5/5
**Precision: 70%**

Results:
1. [8.8/10] Solving positions as deliberate practice → y
2. [8.2/10] Move-selection method for better decisions → y
3. [7.8/10] Concrete line example (single insight) → p
4. [7.4/10] Developing positional judgement (descriptive) → p
5. [6.9/10] Core positional concepts (not actionable) → p

Issues: Results 3-5 are examples or conceptual, not training methods

---

Query 9: When is it correct to sacrifice material?
Manual Evaluation: [y, y, y, y, y] = 5.0/5
**Precision: 100%** ✓✓✓

Results:
1. [9.5/10] Cost-benefit criteria for queen sacrifices → y
2. [9.0/10] Structured evaluation process for sacrifice decisions → y
3. [8.5/10] Rationale via local superiority criterion → y
4. [8.5/10] When pawn sacrifices are justified - clear conditions → y
5. [8.5/10] Exchange sacrifices - compensatory factors → y

**Perfect results** - all 5 results directly answer the query

---

Query 10: How do I convert a winning position?
Manual Evaluation: [y, y, p, y, p] = 4.0/5
**Precision: 80%**

Results:
1. [8.5/10] Generalizable method for king maneuvering → y
2. [8.2/10] Concrete route to Lucena - winning technique → y
3. [8.0/10] Why converting is difficult (conceptual, not steps) → p
4. [7.6/10] Outflanking manoeuvre - clear technique → y
5. [7.5/10] Endgame strategy principles (not procedural) → p

Issues: Results 3 and 5 are conceptual, not actionable steps

================================================================================
DETAILED SCORING SUMMARY
================================================================================

**By Precision Level:**
- 100%: Q4, Q9 (2 queries) = 20%
- 90%: Q7 (1 query) = 10%
- 80%: Q3, Q5, Q6, Q10 (4 queries) = 40%
- 70%: Q1, Q2, Q8 (3 queries) = 30%
- Below 70%: None = 0%

**Total: 82.0% average**

**Distribution:**
- 10 queries × 5 results = 50 total results
- y (relevant): 33 results = 66%
- p (partial): 17 results = 34%
- n (not relevant): 0 results = 0%

================================================================================
KEY FINDINGS
================================================================================

**1. Two-Stage Reranking Performance:**
- Overall: 82% (-4 to -5pp vs baseline)
- Better than previous optimization attempts (+2pp over dual-query fusion)
- Still significantly below ≥88% target (-6pp)
- Does NOT achieve the goal of improving over baseline

**2. Query 5 Regression Analysis:**
- Precision: 80% (vs 90% baseline = -10pp regression)
- Better than unconstrained expansion (60%) and dual-query fusion (80%)
- Critical result #2 ("Amount of study for openings") was preserved ✓
- BUT results 4-5 are book recommendations, not general study methods
- Intent boosting helped but was not sufficient

**3. Strengths:**
- Q4 (creating weaknesses): 100% - perfect results
- Q9 (sacrifice material): 100% - perfect results
- Q7 (endgame principles): 90% - strong results
- No queries fell below 70%
- Intent-aware boosting worked for some query types

**4. Weaknesses:**
- Q1, Q2, Q8: All at 70% (likely regressions from baseline)
- Q5: Still -10pp below baseline
- Many queries returned conceptual/example content vs actionable methods
- Intent boosting insufficient to overcome semantic ranking issues

**5. Root Cause Analysis:**

**Why Two-Stage Reranking Underperformed:**

a) **Semantic Search Candidates (Top 40) Were Suboptimal:**
   - Stage 1 and 2 can only rerank what semantic search retrieves
   - If the top 40 candidates don't contain the best content, reranking can't fix it
   - Query 5: The critical "Amount of study" content was in top 40, but so were many book recommendations

b) **Intent Boosting Rules Were Too Simplistic:**
   - HOW-TO queries: +0.2 for method, -0.1 for theory
   - WHAT-ARE queries: +0.2 for theory
   - These linear boosts don't capture nuanced relevance differences
   - Example: Q1 (calculation improvement) boosted method chunks, but theory chunks about evaluation were partially relevant too

c) **Stage 2 CoT Reasoning Focused on Justification, Not Elimination:**
   - GPT-5 provided detailed justifications for why each chunk was relevant
   - But didn't aggressively filter out partial/tangential content
   - Results with scores 6.5-7.5/10 should have been eliminated but weren't

d) **No Query Expansion Means Limited Recall:**
   - Two-stage reranking operates only on initial semantic search results
   - If semantic search misses critical content, no amount of reranking helps
   - Unlike query expansion (which increases recall but hurts precision), this approach can't discover new content

e) **Intent Labels May Have Been Incomplete:**
   - Only 37% of chunks (213/580) were labeled
   - Many chunks in top 40 may have had default "theory" labels
   - Inaccurate labels → incorrect boosting → suboptimal ranking

**6. Comparison to Previous Attempts:**

| Approach | Precision@5 | vs Baseline | Q5 Result | Key Issue |
|----------|-------------|-------------|-----------|-----------|
| Baseline (Approaches 1+2) | 86-87% | - | 90% | - |
| Unconstrained expansion | 80% | -6 to -7pp | 60% | Intent drift from expansion |
| Constrained (0.75) | 80% | -6 to -7pp | 60% | Same as unconstrained |
| Constrained (0.85) | 86-87% | 0pp | 90% | Zero expansions = baseline |
| Dual-Query Fusion | 80% | -6 to -7pp | 80% | Domain filtering insufficient |
| **Two-Stage Reranking** | **82%** | **-4 to -5pp** | **80%** | **Can't fix poor candidates** |

**7. Why Intent-Aware Boosting Wasn't Enough:**

Despite the sophisticated two-stage approach with intent boosting:
- Intent boosting can only reorder existing candidates
- If the top 40 semantic results are suboptimal, no reranking can fix it
- Query 5: Even with method chunk boosting, book recommendations outranked some pure study methods
- The fundamental issue is semantic search candidate quality, not ranking

================================================================================
DEPLOYMENT DECISION
================================================================================

**Recommendation: DO NOT DEPLOY - Fall back to Approaches 1+2 baseline (86-87%)**

**Rationale:**

1. **Below Baseline**: Two-stage reranking achieves 82%, which is 4-5pp below the 86-87% baseline

2. **Below Target**: The goal was ≥88% precision@5, and 82% is 6pp below target

3. **Query 5 Regression Persists**: At 80% (vs 90% baseline), Query 5 still shows -10pp regression

4. **Effort vs Benefit**:
   - Requires chunk intent labeling (213+ chunks, ongoing maintenance)
   - Requires two GPT-5 calls per query (2× cost, 2× latency)
   - Only achieves +2pp over simpler approaches (dual-query fusion)
   - Still -4 to -5pp below baseline

5. **Risk of Further Regressions**: 3 queries (Q1, Q2, Q8) at 70% suggest possible baseline regressions

6. **Baseline is Proven**: Approaches 1+2 achieve 86-87% with:
   - No query expansion (preserves intent)
   - Single-stage GPT-5 reranking
   - Lower latency and cost
   - Proven stable performance

================================================================================
LESSONS LEARNED
================================================================================

**What Worked:**
1. Intent-aware boosting improved specific queries (Q4, Q9 at 100%)
2. Two-stage reranking provided detailed justifications for results
3. Fast labeling approach (only validation chunks) was efficient
4. No query expansion = no intent drift (vs unconstrained expansion at 60%)

**What Didn't Work:**
1. Reranking can't overcome poor semantic search candidates
2. Simple linear intent boosts (+0.2, -0.1) are insufficient
3. CoT reasoning focused on justification, not aggressive filtering
4. Query 5 regression persists despite intent boosting

**Key Insight:**
The fundamental bottleneck is **semantic search candidate quality**, not reranking. All optimization attempts that operate on top-40 semantic results have hit the same ceiling (~80-82%). To exceed 88%, we would need:
- Better semantic embeddings (different model)
- Hybrid search (semantic + keyword + BM25)
- Query understanding BEFORE retrieval (not just reranking)
- Domain-specific fine-tuned embeddings

**Why Query Expansion Failed:**
Every query expansion approach (unconstrained, constrained, dual-query) caused intent drift:
- "HOW to study openings" → "WHAT are openings"
- Lost critical study methodology content
- Query 5: 90% → 60-80%

**Why Reranking Alone Failed:**
Reranking operates on existing candidates:
- If top 40 doesn't contain best content, reranking can't discover it
- Intent boosting helps but can't overcome ~30-40% suboptimal candidates
- Two-stage reranking: 82% (+2pp over dual-query, -5pp vs baseline)

**Conclusion:**
After 6 different optimization attempts over multiple sessions:
1. Unconstrained expansion: 80% ❌
2. Constrained (0.75): 80% ❌
3. Constrained (0.85): 86-87% (baseline) ✓
4. Dual-Query Fusion: 80% ❌
5. Domain filtering: (not tested standalone)
6. **Two-Stage Reranking: 82%** ❌

**None exceeded the 86-87% baseline.** The baseline remains the best performing approach.

================================================================================
FINAL VERDICT
================================================================================

**Deploy Approaches 1+2 baseline at 86-87% precision@5**

Two-stage reranking with intent-aware boosting does NOT improve over the baseline and should NOT be deployed.

The optimization attempt has reached its conclusion. Further improvements would require fundamental changes to the retrieval architecture (hybrid search, query understanding, domain-specific embeddings), which is beyond the scope of Week 3 validation.

**Status**: Week 3 optimization complete. System A baseline (86-87%) is the recommended deployment configuration.

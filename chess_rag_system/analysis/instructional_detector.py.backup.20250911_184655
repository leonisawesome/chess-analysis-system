"""
Chess-specific instructional language detector - Hotfix implementation
Based on 4-AI partner consultation consensus + category mapping layer fix
Implements: empirical vocabulary, context gates, diminishing returns, slot patterns
FIX-2025-09-11: Category mapping for empirical vocab integration
Rollback: Restore from instructional_detector.py.bak.TIMESTAMP
"""

import re
import logging
import os
import threading
import hashlib
import inspect
import time
import unicodedata as ud
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple, Set
from collections import defaultdict, Counter

# Import vocabulary from hotfix file
from .instructional_vocabulary_hotfix import (
    INSTRUCTIONAL_LEXICON, 
    SLOT_PATTERNS, 
    CATEGORY_WEIGHTS, 
    CATEGORY_CAPS
)

# Phase 1: Safety & Provenance - Log module info on init
logger = logging.getLogger(__name__)
logger.info({
    "detector_file": __file__,
    "lexicon_source": "instructional_vocabulary_hotfix.INSTRUCTIONAL_LEXICON",
    "gate_logic": "OR",
    "gate_window": 1
})

# Phase 2: Category Mapping Layer - Map new names to canonical
# Canonical categories used by weights/caps
CANONICAL_WEIGHTS = {
    "intent": 1.0, "template": 1.0, "endgame": 0.9, "planning": 0.85,
    "tactics": 0.9, "eval": 0.85, "structure": 0.8, "teaching": 0.95, 
    "strategic": 0.8, "opening": 0.75
}
CANONICAL_CAPS = {k: 6 for k in CANONICAL_WEIGHTS}

# Map new empirical names -> canonical
CAT_MAP = {
    "intent_patterns": "intent",
    "planning_patterns": "planning", 
    "tactical_patterns": "tactics",
    "endgame_patterns": "endgame",
    "structure_patterns": "structure",
    "evaluation_patterns": "eval",
    "opening_principle": "opening",
    "teaching_method": "teaching",
    "strategic_concepts": "strategic"
}

def canon(cat: str) -> str:
    """Convert category to canonical name"""
    return CAT_MAP.get(cat, cat)

# Phase 3: Text normalization helper
def _norm(s: str) -> str:
    """Normalize text for pattern matching"""
    s = ud.normalize("NFKC", s).replace("\u00A0", " ").replace("\u00b1", "+/-")
    return re.sub(r"\s+", " ", s).lower()

# Optional embedding support
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    EMBEDDING_AVAILABLE = True
except ImportError:
    EMBEDDING_AVAILABLE = False

# Diagnostic helper functions for EVS multiprocessing debugging
def _sha(s): 
    return hashlib.sha256(str(s).encode()).hexdigest()[:8]

# Global lock for thread-safe compilation
_compile_lock = threading.Lock()

# Removed duplicate logger declaration

@dataclass
class InstructionalHit:
    """Represents a detected instructional phrase with context validation"""
    phrase: str
    category: str
    canonical: str
    position: int
    sentence_idx: int = 0
    context_valid: bool = False
    embedding_score: float = 0.0
    weight: float = 1.0
    from_template: bool = False

class ChessEntityDetector:
    """
    Detects chess context for gate validation
    Implements OR logic across chess cues per AI partner recommendations
    """
    
    def __init__(self):
        # SAN notation patterns
        self.san_pattern = re.compile(
            r'\b(?:O-O(?:-O)?|[KQRBN]?[a-h]?[1-8]?x?[a-h][1-8](?:=[QRBN])?[+#]?)\b'
        )
        
        # Square notation
        self.square_pattern = re.compile(r'\b[a-h][1-8]\b')
        
        # ECO codes
        self.eco_pattern = re.compile(r'\b[A-E][0-9]{2}\b')
        
        # Evaluation symbols and words
        self.eval_pattern = re.compile(
            r'\b(?:only move|equalize|equalizes|advantage|edge|keeps?|consolidates?|press|hold|winning|losing|equal|better|worse|±|∓|==|\+=|\+−)\b',
            re.IGNORECASE
        )
        
        # Chess terminology from existing system
        self.chess_terms_pattern = re.compile(
            r'\b(?:pawn|knight|bishop|rook|queen|king|castle|castling|endgame|opening|middlegame|tactics|strategy|sacrifice|pin|fork|skewer|discovered|deflection|clearance|zwischenzug)\b',
            re.IGNORECASE
        )
    
    def has_chess_context(self, text: str, window_sentences: int = 1) -> bool:
        """
        Check if text has sufficient chess context for instructional phrases
        Uses OR logic across multiple chess indicators per AI partner consensus
        """
        # Check current sentence
        if self._has_chess_indicators(text):
            return True
            
        # Check ±window_sentences if window > 0
        if window_sentences > 0:
            sentences = text.split('.')
            for i, sentence in enumerate(sentences):
                if i > 0 and self._has_chess_indicators(sentences[i-1]):
                    return True
                if i < len(sentences) - 1 and self._has_chess_indicators(sentences[i+1]):
                    return True
                    
        return False
    
    def _has_chess_indicators(self, sentence: str) -> bool:
        """Comprehensive chess context detection with SAN notation support"""
        # Strong indicators (any one passes)
        strong_indicators = [
            self.san_pattern.search(sentence),      # Nf3, d4, etc.
            self.eco_pattern.search(sentence),      # A00-E99
            'castles' in sentence.lower(),
            'en passant' in sentence.lower()
        ]
        
        if any(strong_indicators):
            return True
        
        # Soft indicators (need 1+ for pass - more permissive)
        soft_hits = {
            "chess_terms": bool(self.chess_terms_pattern.search(sentence)),
            "eval_symbols": bool(self.eval_pattern.search(sentence)),
            "squares": bool(self.square_pattern.search(sentence)),
            "improve_piece": "improve" in sentence.lower() and "piece" in sentence.lower(),
            "planning": "plan" in sentence.lower() or "strategy" in sentence.lower(),
            "chess_concepts": any(term in sentence.lower() for term in [
                "attack", "defense", "material", "position", "move", "game",
                "white", "black", "center", "kingside", "queenside", "file", 
                "rank", "diagonal", "weakness", "strength"
            ])
        }
        
        soft_count = sum(soft_hits.values())
        decision = soft_count >= 1  # More permissive threshold
        
        # Store debug info for troubleshooting
        self._last_gate_debug = {
            "strong_triggers": [name for name, hit in zip(
                ["san", "eco", "castles", "en_passant"], strong_indicators) if hit],
            "soft_hits": {k: v for k, v in soft_hits.items() if v},
            "soft_count": soft_count,
            "decision": decision,
            "text_sample": sentence[:60]
        }
        
        return decision
    def _check_soft_chess_consensus(self, text: str) -> bool:
        """Check for 2+ chess-specific multi-word phrases"""
        import re
        text_lower = text.lower()
        soft_cue_count = 0
        
        # Chess-specific patterns per ChatGPT recommendation
        patterns = [
            r'(dominate|control|fight for|seize|contest) (the )?center',
            r'improve (the )?worst piece',
            r'typical plan',
            r'model game', 
            r'fundamental principle',
            r'minority attack',
            r'pawn chain',
            r'open file',
            r'rook lift',
            r'king safety',
            r'piece activity'
        ]
        
        for pattern in patterns:
            if re.search(pattern, text_lower):
                soft_cue_count += 1
                
        return soft_cue_count >= 2

class InstructionalLanguageDetector:
    """
    Chess-specific instructional language detector implementing 4-AI partner consensus:
    - Empirical vocabulary from GM content analysis
    - Context gates with OR logic and ±1 sentence window  
    - Embedding confirmation (optional)
    - Diminishing returns with category caps
    - Slot template support for dynamic patterns
    """
    
    def __init__(self, embedding_model: Optional[SentenceTransformer] = None, 
                 use_embedding_confirmation: bool = False):
        # AI Partner recommendation: Add comprehensive logging for production debugging
        logger.info(f"DETECTOR INIT: PID={os.getpid()}, ID={id(self)}, embedding_model={embedding_model is not None}")
        
        self.entity_detector = ChessEntityDetector()
        self.embedding_model = embedding_model
        self.use_embedding_confirmation = use_embedding_confirmation
        
        # Add version tag to detect cached instances
        self._version = 'v2.1-empirical-lazy'
        
        # Load vocabulary first
        self._load_instructional_vocabulary()
        logger.info(f"DETECTOR VOCAB LOADED: PID={os.getpid()}, fixed_phrases_len={len(self.fixed_phrases) if hasattr(self, 'fixed_phrases') else 0}")
        
        # Phase 3: Per-pattern compilation (kill mega-regex)
        self._compile_patterns()
        self._assert_categories()
        
        # Precompute didactic exemplar embeddings if available
        if self.embedding_model and EMBEDDING_AVAILABLE and use_embedding_confirmation:
            self._precompute_didactic_exemplars()
        
        # Phase 1: Log compiled pattern counts
        lex_counts = {canon(c): sum(1 for cat,_,_ in self._compiled if canon(cat)==canon(c))
                     for c,_,_ in self._compiled}
        logger.info({
            "compiled_patterns": len(self._compiled),
            "lex_counts": lex_counts,
            "embedding_confirmation": self.use_embedding_confirmation
        })
    
    def _load_instructional_vocabulary(self):
        """Load empirical vocabulary from AI partner consultation"""
        self.fixed_phrases = INSTRUCTIONAL_LEXICON
        self.slot_patterns = SLOT_PATTERNS
        self.category_weights = CATEGORY_WEIGHTS  
        self.category_caps = CATEGORY_CAPS
        
        # Log vocabulary statistics per AI partner recommendations
        for category, phrases in self.fixed_phrases.items():
            logger.debug(f"Loaded {len(phrases)} phrases for category '{category}': {phrases[:3]}...")
    
    def _probe(self, where):
        """Diagnostic probing to identify EVS execution context"""
        try:
            import multiprocessing
            start_method = getattr(multiprocessing, "get_start_method", lambda: None)()
        except:
            start_method = "unknown"
            
        logger.info({
            "where": where,
            "pid": os.getpid(),
            "tid": threading.get_ident(),
            "detector_id": id(self),
            "detector_version": getattr(self, "_version", "unknown"),
            "compiled_count": getattr(self, "_compiled", None) and len(self._compiled),
            "lex_loaded": bool(getattr(self, "fixed_phrases", None)),
            "lex_sha": _sha(getattr(self, "fixed_phrases", {})),
            "start_method": start_method,
            "backend": type(getattr(self, "_pool_backend", None)).__name__ if hasattr(self, "_pool_backend") else None,
            "file_path": __file__
        })
    
    def _compile_patterns(self):
        """Phase 3: Per-pattern compilation (kill mega-regex) - Fixed for multiprocessing"""
        self._compiled = []
        
        # Check if vocabulary was loaded
        source = getattr(self, "fixed_phrases", None)
        if not source:
            raise RuntimeError("Lexicon not loaded: fixed_phrases is empty - call _load_instructional_vocabulary() first")
        
        logger.info(f"Starting pattern compilation from {len(source)} categories in PID {os.getpid()}")
        
        # Compile each pattern individually with category tracking
        compiled_count = 0
        for cat, patterns in source.items():
            if not patterns:
                logger.warning(f"Empty patterns for category '{cat}'")
                continue
                
            logger.debug(f"Compiling category '{cat}': {len(patterns)} patterns")
            for p in patterns:
                try:
                    # Escape and compile individual pattern - Fixed word boundaries
                    escaped = re.escape(p.lower())
                    # Fix: Use r'\b' not r'\\b' for word boundaries
                    pattern_str = r'\b' + escaped + r'\b'
                    r = re.compile(pattern_str, re.IGNORECASE)
                    self._compiled.append((cat, r, p))
                    compiled_count += 1
                    logger.debug(f"Compiled pattern '{p}' -> {pattern_str}")
                except re.error as e:
                    logger.error(f"Regex compilation failed in category '{cat}' for pattern '{p}': {e}")
                    continue  # Skip bad pattern, don't break entire compilation
        
        if compiled_count == 0:
            raise RuntimeError(f"No patterns compiled from {len(source)} categories - check lexicon data")
            
        logger.info(f"Pattern compilation complete: {compiled_count} patterns compiled in PID {os.getpid()}")
        
        # Compile slot patterns with template marking
        self.compiled_slot_patterns = []
        for pattern_spec in SLOT_PATTERNS:
            try:
                compiled = re.compile(pattern_spec['pattern'], re.IGNORECASE)
                self.compiled_slot_patterns.append({
                    'category': pattern_spec['category'],
                    'pattern': compiled,
                    'weight': pattern_spec['weight'],
                    'slots': pattern_spec.get('slots', [])
                })
            except re.error as e:
                logger.error(f"Bad slot pattern: {pattern_spec['pattern']} -> {e}")
    
    def _assert_categories(self):
        """Validate all categories map to canonical weights"""
        canon_missing = sorted({canon(c) for c,_,_ in self._compiled} - CANONICAL_WEIGHTS.keys())
        if canon_missing:
            raise KeyError(f"Unknown categories after mapping: {canon_missing}")
    
    def analyze_instructional_language(self, chunks: List[str], debug: bool = False) -> float:
        """
        Phase 4: Sentence-level gating with category mapping
        Returns normalized instructional language score [0.0, 1.0]
        LAZY COMPILATION: Self-healing for multiprocessing contexts
        """
        if not chunks:
            return 0.0
        
        # LAZY COMPILATION GUARDRAIL: Defensive pattern for multiprocessing
        if not hasattr(self, '_compiled') or not self._compiled:
            logger.warning(f"LAZY RECOMPILE TRIGGERED: PID={os.getpid()}, ID={id(self)} - patterns not compiled, compiling just-in-time")
            try:
                self._compile_patterns()
                self._assert_categories()
                logger.info(f"JIT compilation successful: {len(self._compiled)} patterns ready in PID {os.getpid()}")
            except Exception as e:
                logger.error(f"JIT compilation failed in PID {os.getpid()}: {e}")
                return 0.0  # Fail gracefully
        
        # Combine and normalize input
        text = ' '.join(chunks)
        text_norm = _norm(text)
        
        # Simple sentence splitting (improve later if needed)
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        # Phase 3: Collect raw hits with per-pattern matching
        raw_hits = []
        for i, sent in enumerate(sentences):
            sent_norm = _norm(sent)
            
            # Match individual patterns
            for cat, regex, original_phrase in self._compiled:
                match = regex.search(sent_norm)
                if match:
                    hit = InstructionalHit(
                        phrase=original_phrase,
                        category=cat,
                        canonical=canon(cat),
                        position=match.start(),
                        sentence_idx=i,
                        weight=CANONICAL_WEIGHTS.get(canon(cat), 1.0)
                    )
                    raw_hits.append(hit)
            
            # Match slot templates (auto-pass gate)
            for slot_spec in self.compiled_slot_patterns:
                match = slot_spec['pattern'].search(sent_norm)
                if match:
                    hit = InstructionalHit(
                        phrase=match.group(),
                        category=slot_spec['category'],
                        canonical=canon(slot_spec['category']),
                        position=match.start(),
                        sentence_idx=i,
                        weight=slot_spec['weight'],
                        from_template=True
                    )
                    raw_hits.append(hit)
        
        # Phase 4: Sentence-level gating
        gated_hits = []
        for idx in sorted({h.sentence_idx for h in raw_hits}):
            # Get sentence context for gate
            prev_sent = sentences[idx-1] if idx > 0 else ""
            curr_sent = sentences[idx]
            next_sent = sentences[idx+1] if idx+1 < len(sentences) else ""
            
            # Apply gate once per sentence
            gate_ok = self.entity_detector.has_chess_context(
                f"{prev_sent} {curr_sent} {next_sent}", window_sentences=1
            )
            
            # Process all hits from this sentence
            for hit in [h for h in raw_hits if h.sentence_idx == idx]:
                hit.context_valid = gate_ok or hit.from_template
                if hit.context_valid:
                    gated_hits.append(hit)
        
        # Phase 5: Scoring with canonical categories and diminishing returns
        bucket = defaultdict(list)
        for hit in gated_hits:
            bucket[hit.canonical].append(hit.weight)
        
        # Apply diminishing returns per canonical category
        final_score = 0.0
        for canon_cat, weights in bucket.items():
            cap = CANONICAL_CAPS.get(canon_cat, 6)
            limited_weights = weights[:cap]  # Apply cap
            
            for i, w in enumerate(limited_weights):
                diminishing_factor = 0.8 ** i
                final_score += w * diminishing_factor
        
        # Debug info if requested
        if debug:
            debug_info = {
                "hits_raw": len(raw_hits),
                "hits_after_gates": len(gated_hits),
                "examples": [{"cat": h.canonical, "phrase": h.phrase, "sent_idx": h.sentence_idx} 
                           for h in gated_hits[:5]],
                "last_gate_debug": getattr(self.entity_detector, '_last_gate_debug', None)
            }
            return type('Result', (), {'score': final_score, 'debug': debug_info})()
        
        return final_score
    
    def __getstate__(self):
        """Multiprocessing-safe serialization - don't pickle compiled regex objects"""
        state = self.__dict__.copy()
        # Remove compiled regex objects that can't be pickled reliably
        compiled_count = len(state.get('_compiled', []))
        state.pop('_compiled', None)
        state.pop('compiled_slot_patterns', None)
        logger.info(f"GETSTATE CALLED: PID={os.getpid()}, ID={id(self)}, version={getattr(self, '_version', 'unknown')}, nulling {compiled_count} compiled patterns for MP transfer")
        return state
    
    def __setstate__(self, state):
        """Multiprocessing-safe deserialization - recompile patterns in worker"""
        self.__dict__.update(state)
        logger.info(f"SETSTATE CALLED: PID={os.getpid()}, ID={id(self)}, version={getattr(self, '_version', 'unknown')}, recompiling from fixed_phrases_len={len(getattr(self, 'fixed_phrases', {}))}")
        
        # Ensure required dependencies exist
        if not hasattr(self, 'entity_detector'):
            self.entity_detector = ChessEntityDetector()
            
        # Recompile patterns in the new process
        try:
            self._compile_patterns()
            self._assert_categories()
            logger.info(f"SETSTATE SUCCESS: Recompiled {len(self._compiled)} patterns in worker PID {os.getpid()}")
        except Exception as e:
            logger.error(f"SETSTATE FAILED: Pattern recompilation failed in worker PID {os.getpid()}: {e}")
            self._compiled = []  # Ensure it exists even if empty
            raise

    def detect_plan_chains(self, chunks: List[str]) -> float:
        """
        Detect sequential planning language that indicates structured thinking
        Lightweight implementation for 0.05 weight component per AI partner consensus
        """
        if not chunks:
            return 0.0
            
        text = ' '.join(chunks).lower()
        
        # Plan sequence indicators
        plan_indicators = [
            r'\bfirst\b.*\bthen\b',
            r'\binitially\b.*\bnext\b', 
            r'\bstep \d+\b',
            r'\bafter\b.*\bwe can\b',
            r'\bonce\b.*\bfollowed by\b',
            r'\bthe sequence\b',
            r'\bin order to\b.*\bwe must\b',
            r'\bthe plan is\b.*\bthen\b',
            r'\btypical plan\b.*\bfollowed by\b'
        ]
        
        plan_chain_count = 0
        for pattern in plan_indicators:
            matches = re.findall(pattern, text)
            plan_chain_count += len(matches)
        
        # Normalize by text length
        words = text.split()
        if len(words) < 100:
            return 0.0
            
        plan_density = plan_chain_count / (len(words) / 100.0)  # Per 100 words
        return min(plan_density, 1.0)

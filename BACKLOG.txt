CHESS RAG SYSTEM - BACKLOG
Last Updated: October 30, 2025

Purpose: Active work items for both humans and future Claude instances
Context: Provides current status and next steps for system development

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PRIORITY SYSTEM
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
P0: CRITICAL - Blocks core functionality (must do now)
P1: HIGH - Significant quality/capability impact (do next)
P2: MEDIUM - Important but not blocking (planned work)
P3: LOW - Nice to have, optimization (future consideration)

Note: Time estimates removed per Master Prompt - Principal Architect has
no reliable sense of time passage. Complexity assessed during execution.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ACTIVE ITEMS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

ITEM-013: Automated Testing Infrastructure [P0]
Status: NOT STARTED
Priority: CRITICAL
Blocking: Efficient iteration, scaling work

Description:
  Build automated test harness so Claude Code can run tests without
  human intervention. Currently manual PDF creation is inefficient.
  
  Goal: Claude Code executes ./run_tests.sh → generates report → 
        human validates results afterward (no human interaction during run)

Components Needed:
  1. test_queries.json
     - 20 test queries (10 opening + 10 middlegame)
     - Expected validation criteria per query
  
  2. test_runner.py
     - Loops through queries
     - Calls Flask /query endpoint
     - Saves each response with timestamp
     - No human interaction required
  
  3. validators.py
     - Programmatic validation functions
     - check_opening_signature(response, expected)
     - check_no_sicilian_contamination(response)
     - check_diagram_markers_present(response)
     - check_response_structure(response)
  
  4. run_tests.sh
     - Start Flask if not running
     - Execute test_runner.py
     - Generate validation_report.md
     - Create pass_fail_summary.txt
  
  5. Baseline responses (optional)
     - Known good responses for comparison
     - baseline/query_01.json ... query_20.json

Acceptance Criteria:
  - Claude Code can run: ./run_tests.sh
  - Generates timestamped directory: test_results/YYYYMMDD_HHMMSS/
    - response_01.json through response_20.json
    - validation_report.md (readable summary)
    - pass_fail_summary.txt (quick status)
  - Reports: X/20 PASS or lists failures
  - Runtime: Completes without hanging
  - NO human intervention required during execution
  - Human validates report after completion

Dependencies: ITEM-012 (need baseline to compare against)

Technical Approach:
  - JSON schema for test queries:
    {
      "query": "Explain the Italian Game",
      "expected_signature": "1.e4 e5 2.Nf3 Nc6 3.Bc4",
      "forbidden_patterns": ["1.e4 c5"],
      "must_contain_diagrams": true
    }
  - Python script with requests library
  - Validators return (pass/fail, details)
  - Markdown report generation


ITEM-014: Middlegame Hybrid Solution Implementation [P0]
Status: NOT STARTED (CRITICAL GAP - NEVER IMPLEMENTED!)
Priority: CRITICAL
Blocking: Middlegame functionality

Description:
  The ORIGINAL problem that started everything:
  - Middlegame queries not producing diagrams
  - Partner consult (ChatGPT, Grok, Claude) suggested solutions
  - Hybrid approach designed but NEVER implemented
  - Got sidetracked by ITEM-008 and ITEM-011
  
  Current State:
  - canonical_fens.json exists with ~10 concepts
  - query_classifier.py detects middlegame queries
  - Canonical FEN injected into synthesis context
  - BUT: GPT-5 NOT explicitly instructed to use it for diagrams
  
  What's Missing:
  - GPT-5 synthesis prompts need explicit canonical FEN instructions
  - System must generate diagrams FROM canonical FEN
  - Validation that middlegame queries produce relevant diagrams

Acceptance Criteria:
  - Middlegame query: "Explain minority attack"
    * Response contains [DIAGRAM: ...] markers
    * Diagrams show positions relevant to minority attack
    * Uses canonical FEN as starting point/reference
    * No generic opening positions
  
  - Test 10 middlegame concepts from canonical_fens.json
  - 100% diagram generation rate (currently 0%)
  - Diagrams match the middlegame concept being explained

Dependencies: 
  - ITEM-013 (need testing infrastructure to validate)

Technical Approach:
  1. Update synthesis_pipeline.py system prompts:
     - stage2_expand_sections() system_prompt
     - Add explicit instruction about canonical positions
     - "When [CANONICAL POSITION: FEN] is provided, generate diagrams
        that show this position and variations from it"
  
  2. Modify stage2_expand_sections() to detect canonical FEN:
     - If canonical_fen in context:
       - Add to section_prompt: "Use canonical position: {fen}"
       - Instruct GPT-5 to generate diagrams from this position
  
  3. Test with middlegame queries:
     - Minority attack
     - Isolated queen pawn
     - Backward pawn
     - Hanging pawns
     - Maroczy Bind
     - Opposite-side castling attacks
  
  4. Validate diagrams generated correctly:
     - Use automated validators from ITEM-013
     - Check diagram positions match middlegame concept

Notes:
  - This addresses the ROOT CAUSE that started the entire project
  - Cannot consider system "complete" until this works
  - May require partner consult if initial approach fails


ITEM-015: Comprehensive Regression Testing [P1]
Status: NOT STARTED
Priority: HIGH
Blocking: Confidence in system stability

Description:
  After ITEM-014 (middlegame implementation), run comprehensive
  regression test to ensure:
  - All opening queries still work (no regressions from middlegame)
  - All middlegame queries now work (ITEM-014 successful)
  - No new contamination introduced
  - ITEM-008 fix still working
  - Performance hasn't degraded significantly

Test Suite:
  Opening Queries (10):
  1. Italian Game
  2. Ruy Lopez
  3. King's Indian Defense
  4. Caro-Kann Defense
  5. Sicilian Defense
  6. Najdorf Variation
  7. Dragon Variation
  8. Queen's Gambit Accepted
  9. Queen's Gambit Declined
  10. French Defense (Winawer)
  
  Middlegame Queries (10):
  1. Minority attack
  2. Isolated queen pawn
  3. Backward pawn
  4. Hanging pawns
  5. Opposite-side castling
  6. Maroczy Bind
  7. Dragon vs Najdorf comparison (middlegame structures)
  8. Rook endgames
  9. Pawn breaks in closed positions
  10. Weak color complex

Acceptance Criteria:
  - 20/20 queries pass automated validation
  - Opening queries:
    * Correct opening signatures present
    * No Sicilian contamination
    * Diagram markers present
  - Middlegame queries:
    * Diagram markers present
    * Canonical FEN used/referenced
    * Diagrams show middlegame positions (not openings)
  - Generate comprehensive validation report
  - All tests automated via ITEM-013 infrastructure

Dependencies: ITEM-013, ITEM-014

Success Metric:
  - 100% pass rate on both opening and middlegame queries
  - System proven stable after middlegame implementation


ITEM-016: Scale to 5M Chunks - Architecture Planning [P1]
Status: NOT STARTED
Priority: HIGH
Blocking: Production scaling decisions

Description:
  Current System: 357,957 chunks from 1,052 books
  Target: 5,000,000 chunks (14x growth)
  
  Questions to Answer:
  - Can Qdrant handle 5M vectors in single collection?
  - Memory requirements for 5M × 3072-dim vectors?
  - Query latency impact at 5M scale?
  - Should we shard by opening/middlegame/endgame?
  - Index optimization strategies?
  - Hardware requirements (RAM, disk, CPU)?
  - Cost projections (Qdrant Cloud vs self-hosted)?

Acceptance Criteria:
  - Written document with architectural recommendations
  - Memory/storage calculations with formulas
  - Query performance projections (current vs 5M)
  - Sharding strategy proposal (if needed)
  - Migration plan from 357K → 5M chunks
  - Cost analysis (infrastructure + maintenance)
  - Risk assessment and mitigation strategies

Dependencies: 
  - None (planning only, doesn't require working system)

Approach:
  - Research Qdrant documentation on scale limits
  - May require partner consult for architectural decisions
  - Calculate: 5M vectors × 3072 dims × 4 bytes = memory needed
  - Consider distributed Qdrant if single-node insufficient

Notes:
  - User mentioned latency concerns (3+ minutes actual wait time)
  - Need to understand what contributes to latency
  - May discover scaling issues during planning


ITEM-017: Docker Deployment Setup [P2]
Status: NOT STARTED
Priority: MEDIUM

Description:
  Containerize the application for easier deployment and
  environment consistency across development/production.
  
Components:
  1. Dockerfile for Flask application
     - Python 3.11+ base image
     - Install dependencies (requirements.txt)
     - Copy application code
     - Expose port 5001
  
  2. docker-compose.yml for multi-service setup
     - Flask app service
     - Qdrant service (separate container)
     - Network configuration
     - Volume management
  
  3. Volume management
     - Qdrant vector DB persistence
     - Configuration files
     - Logs directory
  
  4. Environment variable configuration
     - OPENAI_API_KEY
     - QDRANT_PATH
     - COLLECTION_NAME
  
  5. Documentation
     - Update README.md with Docker instructions
     - docker-compose up quick start

Acceptance Criteria:
  - docker-compose up starts entire system
  - System works identically to local non-Docker setup
  - All 20 test queries pass in Docker environment
  - Qdrant data persists across container restarts
  - Easy to deploy to cloud (AWS ECS, GCP Cloud Run, etc.)
  - Clear README instructions for Docker deployment

Dependencies: 
  - ITEM-012, ITEM-015 (validate system works before containerizing)

Notes:
  - Don't Dockerize until system proven stable
  - Easier deployment but adds complexity


ITEM-018: Performance Optimization [P2]
Status: NOT STARTED
Priority: MEDIUM

Description:
  User reports queries taking 3+ minutes via website (actual wait time).
  Server-side logs show ~34s internal processing, suggesting significant
  gap between internal timing and user experience.
  
  Need to investigate:
  - What accounts for the 3+ minute total time?
  - Is it OpenAI API calls? (user's hypothesis)
  - Is it multiple synthesis stages?
  - Is it network latency?
  - Is it Qdrant search time?
  - Is it diagram processing?
  
  Current understanding:
  - Server logs: ~34s (embedding + search + rerank + synthesis + diagrams)
  - User experience: 3+ minutes (180+ seconds)
  - Gap: ~146+ seconds unaccounted for
  
  This is a CRITICAL UX problem - 3 minutes is unacceptable wait time.

Investigation Required:
  - Add comprehensive timing instrumentation
  - Track every component end-to-end:
    * Request arrival to Flask
    * Embedding generation time
    * Qdrant search time
    * GPT-5 reranking time (100 candidates)
    * Stage 1 synthesis time
    * Stage 2 synthesis time (per section)
    * Stage 3 synthesis time
    * Diagram marker extraction time
    * Response serialization time
    * Response transmission time
  - Identify where the 146+ seconds is going
  - Profile OpenAI API call latency specifically

Potential Optimizations (after profiling):
  - Cache embeddings for repeated queries
  - Reduce GPT-5 reranking candidates (100 → 50?)
  - Parallel section generation in stage2
  - Reduce synthesis token usage (shorter prompts)
  - Cache synthesis results for identical queries
  - Stream response to user incrementally?

Acceptance Criteria:
  - Root cause of 3+ minute latency identified
  - Documented performance analysis with timing breakdown
  - Implemented optimizations (if feasible without quality loss)
  - Demonstrated query time reduction
  - NO quality degradation (maintain 100% test pass rate)
  - User-perceived latency measurably improved

Dependencies: 
  - ITEM-015 (need stable baseline for comparison)

Notes:
  - May require partner consult if root cause unclear
  - This is a critical UX issue, not just optimization
  - User hypothesis about OpenAI API calls may be correct
  - Need actual measurements before implementing changes


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
COMPLETED ITEMS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

ITEM-001: Sicilian Contamination Bug Discovery [RESOLVED]
Completed: September 2024
Resolution: Led to ITEM-008
Details: Discovered GPT-4o generating Sicilian Defense diagrams
         for non-Sicilian openings (Italian, Ruy Lopez, etc.)

ITEM-008: Regeneration Feedback Loop Solution [COMPLETE]
Completed: October 2025
Result: 100% success rate, 0% Sicilian contamination
Implementation: opening_validator.py with automatic retry logic
Validation: 10/10 test queries passing

ITEM-011: Monolithic Refactoring [COMPLETE]
Completed: October 30, 2025
Result: app.py reduced 1,474 → 262 lines (-82.2%)
Modules Created:
  - chess_positions.py (295 lines)
  - diagram_processor.py (187 lines)
  - opening_validator.py (390 lines)
  - synthesis_pipeline.py (292 lines)
  - rag_engine.py (215 lines)
Status: All modules on GitHub (main branch)
Git: SSH authentication working, repository cleaned (<1MB)

ITEM-012: Functional Testing of Refactored System [VALIDATED]
Completed: October 30, 2025
Result: System validated with test query - refactoring successful
Test Query: "Explain the Italian Game opening"
Response Time: 2 minutes 29 seconds
Validation:
  ✅ Valid JSON response
  ✅ Answer field present
  ✅ No errors or crashes
  ✅ System works after ITEM-011 refactoring
Compatibility Fixes Applied:
  - Python 3.9 type annotations (opening_data.py)
  - OpenAI package: 1.12.0 → >=1.50.0
  - Qdrant version: 1.7.0 → 1.15.1 (matched database version)
  - Added httpx>=0.27.0 dependency
Status: VALIDATED (1/10 queries tested - system proven functional)
Note: Full 10-query test suite available in ITEM_012_functional_test.sh
      but not run (would take 25-30 minutes). System validated as working.


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
FUTURE CONSIDERATIONS (P3)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Items for future partner consults when relevant:

Testing & Quality:
- Unit tests for all 6 modules (pytest)
- Integration tests for pipeline
- Load testing for scale validation
- Code coverage tracking (pytest-cov)
- Type hints throughout codebase
- Docstring completeness audit

Infrastructure:
- CI/CD pipeline (automated testing on git push)
- Monitoring & alerting (uptime, errors, performance)
- Cost tracking dashboard
- API rate limiting
- Caching layer (Redis?)

Features:
- Endgame query support
- Tactics puzzle queries
- Game analysis (user uploads PGN)
- Multi-language support
- User authentication
- Query history/analytics

Documentation:
- API documentation (OpenAPI/Swagger spec)
- Architecture diagrams (visual system design)
- Video tutorials for setup

UI/UX:
- Mobile responsive design
- Progressive web app (PWA)
- Improved loading states
- Query suggestions/autocomplete

Note: These are considerations for when core functionality proven.
      Will evaluate via partner consults as needed.


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
NOTES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Master Prompt Reminders:
- Never estimate time (Principal Architect has no time sense)
- Always validate with logs (never trust "complete" messages)
- ONE block rule for Claude Code commands
- Always ask for approval before proceeding
- Git checkpoint before every change
- Verbose explanations (show reasoning)

Context for New Claude Instances:
- Read README.md first for system architecture
- Read this BACKLOG.txt for current work status
- Read Master Prompt if provided
- Review recent git commits for latest changes

Current Branch: main
Latest Commit: 7a70ad0 (Qdrant version fix 1.7.0 → 1.15.1)
System Status: Refactored and VALIDATED - system working correctly (ITEM-012 ✅)
Critical Gap: Middlegame solution never implemented (ITEM-014)
Critical UX Issue: 3+ minute query latency (ITEM-018)

#!/usr/bin/env python3
"""
System A Web UI
Flask web interface for chess knowledge retrieval with interactive diagrams
"""

import os
import re
import time
import chess
import chess.pgn
import chess.svg
import spacy
from io import StringIO
from flask import Flask, render_template, request, jsonify
from openai import OpenAI
from qdrant_client import QdrantClient
from query_system_a import query_system_a, COLLECTION_NAME, QDRANT_PATH, embed_query, semantic_search, gpt5_rerank, TOP_K, TOP_N
from opening_data import detect_opening


app = Flask(__name__)
app.config['TEMPLATES_AUTO_RELOAD'] = True

# Initialize clients at module level (created ONCE on startup)
print("Initializing clients...")
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable not set!")

OPENAI_CLIENT = OpenAI(api_key=api_key)
QDRANT_CLIENT = QdrantClient(path=QDRANT_PATH)
print(f"‚úì Clients initialized (Qdrant: {QDRANT_CLIENT.count(COLLECTION_NAME).count} vectors)")

# Load spaCy model for smart caption extraction
print("Loading spaCy model...")
try:
    NLP = spacy.load("en_core_web_sm")
    print("‚úì spaCy model loaded")
except OSError:
    print("‚ö†Ô∏è  spaCy model not found. Run: python -m spacy download en_core_web_sm")
    NLP = None


# ============================================================================
# CHESS POSITION DETECTION & PARSING
# ============================================================================

def detect_fen(text: str) -> list:
    """
    Detect FEN strings in text.

    Returns list of (FEN, position_in_text) tuples
    """
    # FEN pattern: 8 ranks separated by slashes, then optional metadata
    # e.g. "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
    fen_pattern = r'\b([rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}\s+[wb]\s+(?:K?Q?k?q?|-)\s+(?:[a-h][36]|-)\s+\d+\s+\d+)\b'

    matches = []
    for match in re.finditer(fen_pattern, text):
        fen = match.group(1)
        try:
            # Validate FEN
            chess.Board(fen)
            matches.append((fen, match.start()))
        except:
            continue

    return matches


def parse_moves_to_fen(moves_text: str, max_moves: int = 20) -> str:
    """
    Parse move notation and compute FEN after the moves.

    IMPORTANT: Only parses complete games starting from move 1.
    Skips mid-game fragments to avoid incorrect positions.

    Args:
        moves_text: Text containing chess moves like "1.e4 e5 2.Nf3 Nc6"
        max_moves: Maximum number of moves to parse

    Returns:
        FEN string after the moves, or None if parsing fails
    """
    try:
        # DEBUG: Log input
        print(f"  [parse_moves_to_fen] Input: {repr(moves_text[:100])}")

        # Clean up moves text
        moves_text = moves_text.strip()

        # Find the start of the game (move 1)
        move_1_match = re.search(r'\b1\.\s*[a-hNBRQKO]', moves_text)
        if not move_1_match:
            print(f"  [parse_moves_to_fen] SKIP: No '1.' found, mid-game fragment")
            return None

        # Extract from move 1 onwards
        game_start = move_1_match.start()
        game_text = moves_text[game_start:]
        print(f"  [parse_moves_to_fen] Found '1.' at position {game_start}")

        # Try parsing as PGN moves
        # Remove move numbers
        cleaned = re.sub(r'\d+\.+', '', game_text)
        # Remove comments
        cleaned = re.sub(r'\{[^}]*\}', '', cleaned)
        cleaned = re.sub(r'\([^)]*\)', '', cleaned)

        # Split into tokens
        tokens = cleaned.split()
        print(f"  [parse_moves_to_fen] Tokens: {tokens[:10]}")

        board = chess.Board()
        move_count = 0
        failed_tokens = []

        for token in tokens:
            if move_count >= max_moves:
                break

            # Skip non-move tokens
            if token in ['1-0', '0-1', '1/2-1/2', '*']:
                break

            try:
                # Try to parse as SAN move
                move = board.parse_san(token)
                board.push(move)
                move_count += 1
            except Exception as e:
                # If parsing fails, skip this token
                failed_tokens.append(f"{token}({str(e)[:20]})")
                continue

        print(f"  [parse_moves_to_fen] Parsed {move_count} moves, {len(failed_tokens)} failed")
        if failed_tokens:
            print(f"  [parse_moves_to_fen] Failed tokens: {failed_tokens[:5]}")

        # Require at least 2 successful moves to avoid garbage
        if move_count >= 2:
            fen = board.fen()
            print(f"  [parse_moves_to_fen] SUCCESS: {fen[:50]}")
            return fen
        else:
            print(f"  [parse_moves_to_fen] FAIL: only {move_count} moves parsed (need ‚â•2)")

    except Exception as e:
        print(f"  [parse_moves_to_fen] EXCEPTION: {str(e)}")
        pass

    return None


def extract_chess_positions(text: str, query: str = "") -> list:
    """
    Extract chess positions from text (FEN strings or move sequences).

    Args:
        text: Text to extract positions from
        query: User's query (for relevance filtering)

    Returns:
        List of dict with 'fen', 'svg', 'caption', 'type', 'lichess_url'
    """
    positions = []
    starting_fen = chess.Board().fen()

    # 1. Detect explicit FEN strings
    fens = detect_fen(text)
    for fen, pos in fens:
        try:
            # Skip starting position (not interesting)
            if fen == starting_fen:
                continue

            board = chess.Board(fen)
            svg = chess.svg.board(board, size=350)

            # Extract FULL context for caption (500 chars)
            caption_start = max(0, pos - 250)
            caption_end = min(len(text), pos + 250)
            caption = text[caption_start:caption_end].strip()

            # Create Lichess URL
            lichess_url = create_lichess_url(fen)

            positions.append({
                'fen': fen,
                'svg': svg,
                'caption': caption,
                'type': 'fen',
                'lichess_url': lichess_url
            })
        except Exception as e:
            print(f"  [extract_chess_positions] Failed to generate SVG for FEN: {e}")
            continue

    # 2. Detect move sequences
    move_number_pattern = r'\b(\d+)\.\s*[a-hNBRQKO]'
    seen_positions = set()  # Track FENs we've already added

    for match in re.finditer(move_number_pattern, text):
        start_pos = match.start()
        end_pos = min(len(text), start_pos + 300)
        search_start = max(0, start_pos - 500)
        context_window = text[search_start:end_pos]

        # Try to parse this window
        fen = parse_moves_to_fen(context_window)
        if fen:
            try:
                # Skip duplicates
                if fen in seen_positions:
                    continue

                # Skip starting position (not interesting)
                if fen == starting_fen:
                    continue

                board = chess.Board(fen)

                # Count moves parsed
                moves_parsed = board.fullmove_number - 1
                if board.turn == chess.BLACK:
                    moves_parsed += 1

                # Require at least 1 full move (2 half-moves)
                if moves_parsed < 1:
                    continue

                seen_positions.add(fen)
                svg = chess.svg.board(board, size=350)

                # Extract FULL context for caption (500 chars around match)
                caption_start = max(0, start_pos - 250)
                caption_end = min(len(text), start_pos + 250)
                caption = text[caption_start:caption_end].strip()

                # Create Lichess URL
                lichess_url = create_lichess_url(fen)

                positions.append({
                    'fen': fen,
                    'svg': svg,
                    'caption': caption,
                    'type': 'moves',
                    'moves_parsed': moves_parsed,
                    'lichess_url': lichess_url
                })

                # Limit to 5 positions max (will filter down further)
                if len(positions) >= 5:
                    break

            except Exception as e:
                print(f"  [extract_chess_positions] Failed to generate SVG for moves: {e}")
                continue

    # 3. Filter by relevance to query
    if query and positions:
        filtered_positions = filter_relevant_positions(positions, query)
        if filtered_positions:
            positions = filtered_positions

    # Limit to top 3 most relevant positions
    return positions[:3]


def filter_relevant_positions(positions: list, query: str) -> list:
    """
    Filter positions by relevance to query.

    For opening queries (e.g., "Sicilian Defense"), only return positions
    that match the opening characteristics.
    """
    query_lower = query.lower()

    # Sicilian Defense: Look for black c5 pawn
    if "sicilian" in query_lower:
        relevant = []
        for pos in positions:
            try:
                board = chess.Board(pos['fen'])
                # Check if c5 has black pawn (Sicilian indicator)
                piece = board.piece_at(chess.C5)
                if piece and piece.piece_type == chess.PAWN and piece.color == chess.BLACK:
                    relevant.append(pos)
            except:
                continue
        if relevant:
            return relevant

    # French Defense: Look for black e6 pawn and white e4 pawn
    elif "french" in query_lower:
        relevant = []
        for pos in positions:
            try:
                board = chess.Board(pos['fen'])
                e6_piece = board.piece_at(chess.E6)
                e4_piece = board.piece_at(chess.E4)
                if (e6_piece and e6_piece.piece_type == chess.PAWN and e6_piece.color == chess.BLACK and
                    e4_piece and e4_piece.piece_type == chess.PAWN and e4_piece.color == chess.WHITE):
                    relevant.append(pos)
            except:
                continue
        if relevant:
            return relevant

    # For other queries, return all positions
    return positions


def create_lichess_url(fen: str) -> str:
    """
    Create Lichess analysis URL from FEN.

    Returns:
        Lichess analysis URL
    """
    # URL encode the FEN
    import urllib.parse
    fen_encoded = urllib.parse.quote(fen)
    return f"https://lichess.org/analysis/{fen_encoded}"


# ============================================================================
# MULTI-STAGE SYNTHESIS PIPELINE
# ============================================================================

import json

def stage1_generate_outline(openai_client, query: str, top_chunks: list) -> dict:
    """
    Stage 1: Generate structured outline from query and top chunks.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        top_chunks: List of top 5 relevant text chunks

    Returns:
        dict with sections: {'sections': [{'title': ..., 'bullets': [...]}]}
    """
    # Combine top chunks into context
    context = "\n\n---\n\n".join([chunk[:800] for chunk in top_chunks[:5]])

    prompt = f"""You are a chess instructor creating a comprehensive guide. Based on the user's question and reference material, create a structured outline.

User Question: {query}

Reference Material:
{context}

Create a JSON outline with 5-7 sections. Each section should have:
- title: A clear section heading (e.g., "Overview", "Strategic Themes", "Main Variations", "Key Plans", "Common Tactics")
- bullets: 3-5 bullet points summarizing key information for that section
- diagram_anchor: A standard diagram position for this section (see templates below)

DIAGRAM ANCHOR TEMPLATES:
When creating the outline, specify which standard diagram position to use for each major section:

For Sicilian Defense queries, use these anchors:
- Introduction section: [ANCHOR: Basic Sicilian - 1.e4 c5]
- Najdorf section: [ANCHOR: Najdorf - 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6]
- Dragon section: [ANCHOR: Dragon - 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 g6]
- Sveshnikov section: [ANCHOR: Sveshnikov - 1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 e5]
- Strategic themes section: [ANCHOR: Typical Sicilian pawn structure]

For each section in your JSON outline, include a "diagram_anchor" field with the appropriate anchor.

Return ONLY valid JSON in this exact format:
{{
  "sections": [
    {{"title": "Overview", "bullets": ["...", "...", "..."], "diagram_anchor": "[ANCHOR: Basic Sicilian - 1.e4 c5]"}},
    {{"title": "Strategic Themes", "bullets": ["...", "..."], "diagram_anchor": "[ANCHOR: Typical Sicilian pawn structure]"}},
    ...
  ]
}}"""

    response = openai_client.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=1500
    )

    try:
        raw_response = response.choices[0].message.content

        # Strip markdown code blocks if present
        if "```json" in raw_response:
            raw_response = raw_response.split("```json")[1].split("```")[0]
        elif "```" in raw_response:
            raw_response = raw_response.split("```")[1].split("```")[0]

        outline = json.loads(raw_response.strip())
        print(f"[Stage 1] ‚úÖ Generated outline with {len(outline['sections'])} sections")
        return outline

    except json.JSONDecodeError as e:
        print(f"[Stage 1] ‚ùå JSON parse error: {e}")
        print(f"[Stage 1] Raw response: {response.choices[0].message.content[:500]}...")
        return {"sections": [{"title": "Overview", "bullets": ["General information about " + query]}]}
    except Exception as e:
        print(f"[Stage 1] ‚ùå Unexpected error: {e}")
        print(f"[Stage 1] Raw response: {response.choices[0].message.content[:500]}...")
        return {"sections": [{"title": "Overview", "bullets": ["General information about " + query]}]}


def stage2_expand_sections(openai_client, query: str, sections: list, context: str) -> list:
    """
    Stage 2: Expand each section into 150-300 words with chess notation and diagram markers.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        sections: List of section dicts from stage 1
        context: Combined reference text

    Returns:
        List of expanded section texts
    """
    expanded_sections = []

    for section in sections:
        title = section['title']
        bullets = section['bullets']
        diagram_anchor = section.get('diagram_anchor', '')
        bullets_text = "\n".join([f"- {b}" for b in bullets])

        # Extract move sequence from diagram_anchor
        anchor_moves = ''
        if diagram_anchor and '-' in diagram_anchor:
            anchor_moves = diagram_anchor.split('-', 1)[1].strip().rstrip(']')

        # FIX 1: Enhanced system message with structured validation rules
        system_message = """You are an expert chess instructor creating educational content.

<validation_rules>
CRITICAL: You MUST follow these diagram validation rules:
1. FIRST DIAGRAM: Must start from move 1 and match the opening signature exactly
2. SUBSEQUENT DIAGRAMS: May show continuations from later positions, but MUST be from the same opening family
3. PROHIBITED: Never include diagrams from unrelated openings
4. WHEN IN DOUBT: Omit a diagram rather than risk showing wrong opening
</validation_rules>

<reasoning_process>
Before generating each diagram, verify:
1. What opening am I writing about? (check section title)
2. Does this diagram start with the correct opening moves?
3. If it's a continuation, is it a valid variation of this specific opening?
4. Could this diagram be confused with a different opening family?
</reasoning_process>

<examples>
CORRECT - Italian Game First Diagram:
[DIAGRAM: 1.e4 e5 2.Nf3 Nc6 3.Bc4]
‚úì Starts from move 1
‚úì Matches Italian Game signature

WRONG - Sicilian in Italian Game Article:
[DIAGRAM: 1.e4 c5 2.Nf3 d6]
‚úó This is Sicilian Defense (1.e4 c5)
‚úó Completely different opening - NEVER do this
</examples>"""

        # FIX 2: Add opening signature mapping
        opening_signatures = {
            "Italian Game": "1.e4 e5 2.Nf3 Nc6 3.Bc4",
            "Sicilian Defense": "1.e4 c5",
            "French Defense": "1.e4 e6",
            "Caro-Kann Defense": "1.e4 c6",
            "Ruy Lopez": "1.e4 e5 2.Nf3 Nc6 3.Bb5",
            "Queen's Gambit": "1.d4 d5 2.c4",
            "King's Indian Defense": "1.d4 Nf6 2.c4 g6",
            "Nimzo-Indian Defense": "1.d4 Nf6 2.c4 e6 3.Nc3 Bb4",
            "English Opening": "1.c4",
            "Catalan Opening": "1.d4 Nf6 2.c4 e6 3.g3"
        }

        # Detect which opening we're discussing
        detected_opening = None
        for opening_name, signature in opening_signatures.items():
            if opening_name.lower() in title.lower():
                detected_opening = opening_name
                break

        opening_context = ""
        if detected_opening:
            opening_context = f"\n\nOPENING SIGNATURE: {detected_opening} = {opening_signatures[detected_opening]}\nAll diagrams MUST start with these moves or be continuations of this sequence."

        prompt = f"""You are writing a section of a chess guide. Expand the following outline into 150-300 words.

Section Title: {title}
Key Points:
{bullets_text}

DIAGRAM ANCHOR FOR THIS SECTION: {diagram_anchor if diagram_anchor else 'No anchor - generate appropriate diagram'}
EXTRACTED MOVE SEQUENCE: {anchor_moves if anchor_moves else 'Generate move sequence based on content'}

Reference Context (if needed):
{context[:1500]}{opening_context}

Requirements:
- CRITICAL: All diagrams must show positions directly from the requested opening only. Do NOT include unrelated openings or comparisons (e.g., do not show Sicilian Defense positions when asked about Italian Game).
- Write 150-300 words in an educational, engaging style
- Include specific chess moves in algebraic notation (e.g., 1.e4 c5 2.Nf3)
- Be specific and practical
- Focus on understanding, not just memorization

DIAGRAM REQUIREMENTS (MANDATORY):
- Include exactly one [DIAGRAM: move sequence] marker per major variation discussed
- Place it IMMEDIATELY after introducing the key moves
- If EXTRACTED MOVE SEQUENCE is provided above, USE IT EXACTLY for your diagram marker
- Format: "The [variation] arises after [moves]. [DIAGRAM: moves] This position..."

EXAMPLE:
"The Najdorf Variation arises after 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6. [DIAGRAM: 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6] This flexible pawn structure allows Black to prepare queenside expansion."

MINIMUM: At least 1 diagram per major variation section (Najdorf, Dragon, etc.)

Write the expanded section now:"""

        response = openai_client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lowered from 0.4 for more deterministic/accurate output
            max_tokens=800
        )

        expanded = response.choices[0].message.content.strip()

        # DEBUG: Show what GPT-4o actually generated
        print(f"\n{'='*60}")
        print(f"[Stage 2] Section: {title}")
        print(f"[Stage 2] GPT-4o Output Preview:")
        print(expanded[:500])  # First 500 chars
        print(f"[Stage 2] Contains '[DIAGRAM:': {'[DIAGRAM:' in expanded}")
        print(f"{'='*60}\n")

        expanded_sections.append(f"## {title}\n\n{expanded}")
        print(f"[Stage 2] Expanded section: {title} ({len(expanded.split())} words)")

    # FIX 3: Validate diagrams before returning
    print(f"\n[Stage 2 DEBUG] Starting validation...")
    print(f"[Stage 2 DEBUG] Query: {query}")

    # ECO-based opening detection
    detected_opening, expected_signature, eco_code = detect_opening(query)

    if detected_opening is None:
        print(f"[Stage 2 DEBUG] No opening detected in query: '{query}'")
    else:
        print(f"[Stage 2 DEBUG] Detected opening: {detected_opening} (ECO: {eco_code})")

    if detected_opening:
        print(f"[Stage 2 DEBUG] Calling validate_stage2_diagrams for {detected_opening}")
        validation_result = validate_stage2_diagrams(expanded_sections, detected_opening)
        print(f"[Stage 2 DEBUG] Validation result: {validation_result}")

        if not validation_result:
            print(f"[Stage 2] ‚ö†Ô∏è VALIDATION FAILED: Wrong opening diagrams detected for {detected_opening}")
            print(f"[Stage 2] üö® CRITICAL BUG: Contaminated content would reach user")
            print(f"[Stage 2] ‚ùå TODO: Implement section regeneration here")
            print(f"[Stage 2] ‚ö†Ô∏è  For now, continuing with contaminated content (BAD!)")

    return expanded_sections


def validate_stage2_diagrams(expanded_sections: list, opening_name: str) -> bool:
    """
    FIX 3: Validate that all diagram markers match the expected opening signature.

    Args:
        expanded_sections: List of expanded section texts from Stage 2
        opening_name: Name of the opening being discussed (e.g., "Italian Game")

    Returns:
        True if all diagrams match the opening signature, False otherwise
    """
    print(f"\n[Validation] validate_stage2_diagrams called for: {opening_name}")
    print(f"[Validation] Number of sections to validate: {len(expanded_sections)}")

    # Opening signatures for validation
    opening_signatures = {
        "Italian Game": "1.e4 e5 2.Nf3 Nc6 3.Bc4",
        "Sicilian Defense": "1.e4 c5",
        "French Defense": "1.e4 e6",
        "Caro-Kann Defense": "1.e4 c6",
        "Ruy Lopez": "1.e4 e5 2.Nf3 Nc6 3.Bb5",
        "Queen's Gambit": "1.d4 d5 2.c4",
        "King's Indian Defense": "1.d4 Nf6 2.c4 g6",
        "Nimzo-Indian Defense": "1.d4 Nf6 2.c4 e6 3.Nc3 Bb4",
        "English Opening": "1.c4",
        "Catalan Opening": "1.d4 Nf6 2.c4 e6 3.g3"
    }

    # Illegal moves for each opening (for continuation validation)
    illegal_starts = {
        "Sicilian Defense": ["1.d4", "1.c4", "1.Nf3"],
        "Italian Game": ["1.d4", "1.c4", "1.Nf3"],
        "French Defense": ["1.d4", "1.c4", "1.Nf3"],
        "Ruy Lopez": ["1.d4", "1.c4"],
        "King's Indian Defense": ["1.e4"],
        "Queen's Gambit": ["1.e4"],
    }

    if opening_name not in opening_signatures:
        print(f"[Validation] No signature found for opening: {opening_name}")
        return True  # Skip validation for unknown openings

    expected_signature = opening_signatures[opening_name]
    print(f"[Validation] Expected signature: {expected_signature}")
    all_valid = True

    # Extract all diagram markers from expanded sections
    import re
    total_diagrams = 0
    for i, section in enumerate(expanded_sections):
        diagrams = re.findall(r'\[DIAGRAM:\s*([^\]]+)\]', section)

        for diagram_moves in diagrams:
            total_diagrams += 1
            diagram_moves = diagram_moves.strip()

            if total_diagrams == 1:
                # First diagram: STRICT validation - must start from move 1 and match signature
                if not diagram_moves.startswith("1."):
                    print(f"[Validation]   ‚ùå First diagram must start from move 1!")
                    all_valid = False
                elif not diagram_moves.startswith(expected_signature):
                    print(f"[Validation]   ‚ùå First diagram doesn't match opening signature!")
                    print(f"[Validation]      Expected: {expected_signature}")
                    print(f"[Validation]      Got: {diagram_moves[:50]}...")
                    all_valid = False
                else:
                    print(f"[Validation]   ‚úÖ First diagram valid - matches {opening_name}")
            else:
                # Subsequent diagrams: FLEXIBLE validation
                if diagram_moves.startswith("1."):
                    # If it starts from move 1, it must match the signature
                    if diagram_moves.startswith(expected_signature):
                        print(f"[Validation]   ‚úÖ Valid - matches signature")
                    else:
                        print(f"[Validation]   ‚ùå Starts from move 1 but wrong opening!")
                        print(f"[Validation]      Expected: {expected_signature}")
                        print(f"[Validation]      Got: {diagram_moves[:50]}...")
                        all_valid = False
                else:
                    # Continuation diagram - check for contradictions
                    has_contradiction = False
                    for illegal in illegal_starts.get(opening_name, []):
                        if illegal in diagram_moves:
                            print(f"[Validation]   ‚ùå Continuation contains illegal moves: {illegal}")
                            has_contradiction = True
                            all_valid = False
                            break
                    if not has_contradiction:
                        print(f"[Validation]   ‚ö†Ô∏è  Continuation (move {diagram_moves.split('.')[0]}+) - allowing")

    print(f"\n[Validation] Summary: {total_diagrams} total diagram(s) checked")
    if all_valid:
        print(f"[Validation] ‚úÖ All diagrams match opening signature: {opening_name}")
    else:
        print(f"[Validation] ‚ùå Some diagrams do NOT match opening signature!")

    return all_valid


def stage3_final_assembly(openai_client, query: str, expanded_sections: list) -> str:
    """
    Stage 3: Assemble expanded sections into coherent 800-1500 word article.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        expanded_sections: List of expanded section texts

    Returns:
        Final synthesized article text
    """
    sections_text = "\n\n".join(expanded_sections)

    # Count diagrams BEFORE Stage 3
    diagram_count = sections_text.count('[DIAGRAM:')
    print(f"[Stage 3] Input has {diagram_count} diagram markers")

    prompt = f"""You are combining multiple draft sections into a cohesive chess article.

CRITICAL STRUCTURE INTEGRITY REQUIREMENT:
- Input contains {diagram_count} diagram markers in the format [DIAGRAM: ...].
- Every diagram marker MUST appear in the final article exactly as written.
- These markers are structural and cannot be edited, merged, or deleted.
- You may move them for readability but not remove them.
- Output MUST contain exactly {diagram_count} markers.
- If any are missing, your answer is INVALID.

User Question: {query}

Input sections:
{sections_text}

Requirements:
- Add smooth transitions between sections
- Ensure logical flow from basics to advanced concepts
- Add a brief "Study Recommendations" section at the end
- Target length: 800-1500 words
- Preserve all {diagram_count} diagram markers exactly

Now produce the final integrated article:"""

    response = openai_client.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
        max_tokens=2500
    )

    article = response.choices[0].message.content.strip()

    # POST-VALIDATION: Check diagram count
    output_diagram_count = article.count('[DIAGRAM:')
    print(f"[Stage 3] Output has {output_diagram_count}/{diagram_count} markers")

    if output_diagram_count < diagram_count:
        print(f"[Stage 3] ‚ö†Ô∏è VALIDATION FAILED: Missing {diagram_count - output_diagram_count} markers")
        print(f"[Stage 3] Attempting corrective retry...")

        correction_prompt = f"""You failed to preserve all diagram markers in your previous attempt.

ORIGINAL STAGE 3 INPUT (which had {diagram_count} markers):
{sections_text}

CRITICAL STRUCTURE INTEGRITY REQUIREMENT:
- Input contains {diagram_count} diagram markers in the format [DIAGRAM: ...].
- Every diagram marker MUST appear in the final article exactly as written.
- Output MUST contain exactly {diagram_count} markers.
- If any are missing, your answer is INVALID.

Your previous output had only {output_diagram_count} markers (INCORRECT).

Requirements:
- Add smooth transitions between sections
- Ensure logical flow from basics to advanced concepts
- Add a brief "Study Recommendations" section at the end
- Target length: 800-1500 words
- Preserve all {diagram_count} diagram markers exactly

Regenerate the complete article with ALL {diagram_count} markers preserved:"""

        retry_response = openai_client.chat.completions.create(
            model="gpt-5",
            messages=[{"role": "user", "content": correction_prompt}],
            temperature=0.1,
            max_tokens=2500
        )

        article = retry_response.choices[0].message.content.strip()
        final_count = article.count('[DIAGRAM:')
        print(f"[Stage 3] Retry result: {final_count}/{diagram_count} markers")

    word_count = len(article.split())
    print(f"[Stage 3] Final article: {word_count} words")

    return article


def validate_and_fix_diagrams(openai_client, query: str, expanded_sections: list, context: str) -> list:
    """
    Validate that expanded sections contain [DIAGRAM: ...] markers.
    If missing, regenerate sections with stronger diagram enforcement.

    Args:
        openai_client: OpenAI client
        query: User's query
        expanded_sections: List of section texts
        context: Combined reference text

    Returns:
        List of validated section texts with diagrams
    """
    # Check if ANY diagrams exist in the expanded sections
    combined_text = "\n\n".join(expanded_sections)
    diagram_count = combined_text.count('[DIAGRAM:')

    print(f"\n[Validation] Checking diagram generation...")
    print(f"[Validation] Found {diagram_count} diagram markers in expanded sections")

    if diagram_count == 0:
        print(f"[Validation] ‚ö†Ô∏è  NO DIAGRAMS FOUND! Attempting to add diagrams...")

        # Simple fallback: Add a basic diagram to the first section about the opening
        # Extract opening name from query
        query_lower = query.lower()

        # Try to add a relevant diagram based on the query
        if "italian" in query_lower:
            diagram_marker = "[DIAGRAM: 1.e4 e5 2.Nf3 Nc6 3.Bc4]"
            insert_text = f"\n\nThe Italian Game arises after 1.e4 e5 2.Nf3 Nc6 3.Bc4. {diagram_marker} This position is the starting point for many variations.\n"
        elif "sicilian" in query_lower:
            diagram_marker = "[DIAGRAM: 1.e4 c5]"
            insert_text = f"\n\nThe Sicilian Defence begins with 1.e4 c5. {diagram_marker} This is the fundamental position of the opening.\n"
        elif "french" in query_lower:
            diagram_marker = "[DIAGRAM: 1.e4 e6]"
            insert_text = f"\n\nThe French Defence starts with 1.e4 e6. {diagram_marker} This marks the beginning of this solid defensive system.\n"
        else:
            # Generic fallback
            diagram_marker = "[DIAGRAM: starting position]"
            insert_text = f"\n\n{diagram_marker} We'll examine the key positions that arise from this opening.\n"

        # Insert the diagram into the first section (after the title)
        if expanded_sections:
            first_section = expanded_sections[0]
            # Find the first paragraph break after the title
            lines = first_section.split('\n')
            if len(lines) >= 3:
                # Insert after title and first paragraph
                lines.insert(3, insert_text)
                expanded_sections[0] = '\n'.join(lines)
                print(f"[Validation] ‚úÖ Added fallback diagram: {diagram_marker}")
            else:
                expanded_sections[0] = first_section + insert_text
                print(f"[Validation] ‚úÖ Added fallback diagram at end: {diagram_marker}")

    return expanded_sections


def synthesize_answer(openai_client, query: str, top_chunks: list) -> str:
    """
    Complete 3-stage synthesis pipeline.

    Args:
        openai_client: OpenAI client instance
        query: User's query
        top_chunks: List of top ranked text chunks

    Returns:
        Synthesized answer text
    """
    print("\n" + "="*60)
    print("MULTI-STAGE SYNTHESIS PIPELINE")
    print("="*60)

    # Stage 1: Generate outline
    outline = stage1_generate_outline(openai_client, query, top_chunks)

    # Stage 2: Expand sections
    context = "\n\n".join([chunk[:1000] for chunk in top_chunks[:8]])
    expanded_sections = stage2_expand_sections(
        openai_client,
        query,
        outline['sections'],
        context
    )

    # Stage 2.5: Validate diagrams and fix if missing
    expanded_sections = validate_and_fix_diagrams(
        openai_client,
        query,
        expanded_sections,
        context
    )

    # Stage 3: Final assembly
    final_article = stage3_final_assembly(openai_client, query, expanded_sections)

    print("="*60 + "\n")

    return final_article


def extract_moves_from_description(description: str) -> str:
    """
    Extract move sequence from a diagram description.

    Args:
        description: Text like "after 1.e4 c5 2.Nf3 Nc6"

    Returns:
        Move sequence like "1.e4 c5 2.Nf3 Nc6" or empty string
    """
    # Look for patterns like "after 1.e4 c5 2.Nf3"
    match = re.search(r'after\s+((?:\d+\.\s*)?[a-h0-9NBRQKOx+#=-]+(?:\s+(?:\d+\.\s*)?[a-h0-9NBRQKOx+#=-]+)*)', description, re.IGNORECASE)
    if match:
        moves = match.group(1).strip()
        print(f"  [extract_moves_from_description] Input description: {repr(description)}")
        print(f"  [extract_moves_from_description] Extracted moves: {repr(moves)}")
        print(f"  [extract_moves_from_description] Move length: {len(moves)} chars")
        return moves

    # Try direct pattern matching (move numbers with moves)
    match = re.search(r'(\d+\.\s*[a-hNBRQKO][^\]]*)', description)
    if match:
        moves = match.group(1).strip()
        print(f"  [extract_moves_from_description] Extracted (direct): {moves}")
        return moves

    print(f"  [extract_moves_from_description] No moves found in: {description[:50]}")
    return ""


def extract_diagram_markers(synthesized_text: str) -> list:
    """
    Find all [DIAGRAM: ...] markers in text and extract move sequences.

    Args:
        synthesized_text: Text containing [DIAGRAM: ...] markers

    Returns:
        List of dicts with marker, description, fen, svg, lichess_url
    """
    pattern = r'\[DIAGRAM:\s*([^\]]+)\]'
    matches = re.findall(pattern, synthesized_text)

    print(f"\n{'='*60}")
    print(f"EXTRACTING DIAGRAM MARKERS")
    print(f"{'='*60}")
    print(f"Found {len(matches)} diagram markers in synthesis output")

    diagrams = []
    for i, match in enumerate(matches):
        print(f"\n  Diagram {i+1}: {match[:60]}...")

        # Extract section context for caption
        marker_pos = synthesized_text.find(f'[DIAGRAM: {match}]')
        section_title = ''
        if marker_pos > 0:
            # Look backward for nearest ## or ### header
            text_before = synthesized_text[:marker_pos]
            # Prioritize ### (subsection) headers like "### Najdorf Variation"
            subsection_headers = re.findall(r'###\s+([^\n]+)', text_before)
            if subsection_headers:
                section_title = subsection_headers[-1].strip()
            else:
                # Fall back to ## headers if no ### found
                section_headers = re.findall(r'##\s+([^\n]+)', text_before)
                if section_headers:
                    section_title = section_headers[-1].strip()

        # If section title is generic, try to extract variation name from nearby text
        if section_title and section_title in ['Main Variations', 'Strategic Themes', 'Overview']:
            # Look for variation names in text around the diagram
            context_window = synthesized_text[max(0, marker_pos-500):min(len(synthesized_text), marker_pos+100)]

            # Common variation patterns
            variation_patterns = [
                r'(Open Sicilian)',
                r'(Alapin Variation)',
                r'(Closed Sicilian)',
                r'(Grand Prix Attack)',
                r'(Dragon Variation)',
                r'(Najdorf Variation)',
                r'(Sveshnikov Variation)',
                r'(Accelerated Dragon)',
            ]

            for pattern in variation_patterns:
                match_var = re.search(pattern, context_window, re.IGNORECASE)
                if match_var:
                    section_title = match_var.group(1)
                    break

        # Extract brief description following the diagram marker
        description = ''
        marker_end = marker_pos + len(f'[DIAGRAM: {match}]')
        context_after = synthesized_text[marker_end:marker_end + 300]
        # Get first sentence or two
        # Split on sentence boundaries, but not on abbreviations
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', context_after)
        if sentences and len(sentences) >= 2:
            # Take first 2-3 sentences
            description = ' '.join(sentences[:3])
            if not description.endswith(('.', '!', '?')):
                description += '.'
        elif sentences:
            description = sentences[0]
            if not description.endswith(('.', '!', '?')):
                description += '.'

        # Parse the move sequence from the marker
        moves = extract_moves_from_description(match)
        if moves:
            # Try to parse moves to FEN
            fen = parse_moves_to_fen(moves)
            if fen:
                try:
                    board = chess.Board(fen)
                    svg = chess.svg.board(board, size=350)
                    lichess_url = create_lichess_url(fen)

                    # Format: Variation Name > Moves > Description
                    caption_parts = []
                    if section_title:
                        caption_parts.append(section_title)
                    caption_parts.append(match)  # The moves
                    if description:
                        caption_parts.append(description)
                    caption = '\n'.join(caption_parts)

                    diagrams.append({
                        'marker': f'[DIAGRAM: {match}]',
                        'description': match,
                        'fen': fen,
                        'svg': svg,
                        'lichess_url': lichess_url,
                        'caption': caption
                    })
                    print(f"  ‚úÖ Successfully parsed diagram {i+1}")
                except Exception as e:
                    print(f"  ‚ùå Failed to create board for diagram {i+1}: {e}")
            else:
                print(f"  ‚ùå Failed to parse moves for diagram {i+1}")
        else:
            print(f"  ‚ùå No moves extracted from diagram {i+1}")

    print(f"\n{'='*60}")
    print(f"Extracted {len(diagrams)} valid diagrams")
    print(f"{'='*60}\n")

    return diagrams


def replace_markers_with_ids(synthesized_text: str, diagrams: list) -> str:
    """
    Replace [DIAGRAM: ...] with [DIAGRAM_ID_0], [DIAGRAM_ID_1], etc.

    Args:
        synthesized_text: Text containing [DIAGRAM: ...] markers
        diagrams: List of diagram dicts from extract_diagram_markers()

    Returns:
        Text with markers replaced by IDs
    """
    modified_text = synthesized_text
    for i, diagram in enumerate(diagrams):
        marker = diagram['marker']
        placeholder = f'[DIAGRAM_ID_{i}]'
        modified_text = modified_text.replace(marker, placeholder, 1)
        print(f"  Replaced: {marker[:50]}... -> {placeholder}")

    return modified_text


# ============================================================================
# FLASK ROUTES
# ============================================================================

@app.route('/')
def index():
    """Main page."""
    return render_template('index.html')


@app.route('/test', methods=['POST'])
def test():
    """Test endpoint."""
    return jsonify({'status': 'ok', 'message': 'test works'})


@app.route('/query', methods=['POST'])
def query():
    """Handle query requests with detailed timing."""
    print("=" * 80)
    print("QUERY ENDPOINT CALLED")
    print("=" * 80)
    try:
        start = time.time()

        # Step 1: Parse request
        data = request.get_json()
        query_text = data.get('query', '').strip()
        t1 = time.time()
        print(f"‚è±  Request parsing: {t1-start:.2f}s")

        if not query_text:
            return jsonify({'error': 'Query cannot be empty'}), 400

        # Step 2: Generate embedding
        query_vector = embed_query(OPENAI_CLIENT, query_text)
        t2 = time.time()
        print(f"‚è±  Embedding: {t2-t1:.2f}s")

        # Step 3: Search Qdrant
        candidates = semantic_search(QDRANT_CLIENT, query_vector, top_k=TOP_K)
        t3 = time.time()
        print(f"‚è±  Qdrant search: {t3-t2:.2f}s")

        # Step 4: Rerank with GPT-5
        ranked_results = gpt5_rerank(OPENAI_CLIENT, query_text, candidates, top_k=TOP_N)
        t4 = time.time()
        print(f"‚è±  GPT-5 reranking: {t4-t3:.2f}s")

        # Step 5: Format results for web display
        results = []
        for candidate, score in ranked_results:
            payload = candidate.payload

            # Extract metadata
            book_name = payload.get('book_name', 'Unknown')
            if book_name.endswith('.epub') or book_name.endswith('.mobi'):
                book_name = book_name[:-5]

            text = payload.get('text', '')
            chapter = payload.get('chapter_title', '')

            # Extract chess positions from text (pass query for relevance filtering)
            positions = extract_chess_positions(text, query=query_text)

            # Format result
            result = {
                'score': round(score, 1),
                'book_name': book_name,
                'book': book_name,  # Keep for backwards compatibility
                'chapter_title': chapter,
                'chapter': chapter,  # Keep for backwards compatibility
                'text': text,
                'positions': positions,
                'has_positions': len(positions) > 0
            }
            results.append(result)

        t5 = time.time()
        print(f"‚è±  Response formatting: {t5-t4:.2f}s")

        # DEBUG: Check position extraction
        print("\n=== POSITION EXTRACTION DEBUG ===")
        for i, result in enumerate(results[:5]):
            chunk_text = result.get('text', '')
            print(f"\nSource {i+1}:")
            print(f"  Book: {result.get('book_name', 'unknown')}")
            print(f"  Text preview: {chunk_text[:100]}...")

            # Check what position data exists
            if 'positions' in result:
                print(f"  ‚úÖ Positions array length: {len(result['positions'])}")
                if len(result['positions']) > 0:
                    print(f"     First position: {result['positions'][0]}")
            else:
                print(f"  ‚ùå No 'positions' key")

            if 'has_positions' in result:
                print(f"  has_positions flag: {result['has_positions']}")
            else:
                print(f"  ‚ùå No 'has_positions' key")

            # Try to detect patterns in text
            import re
            fen_pattern = r'[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}/[rnbqkpRNBQKP1-8]{1,8}'
            moves_pattern = r'1\.\s*e4\s+c5'

            if re.search(fen_pattern, chunk_text):
                print(f"  üìã FEN detected in text")
            if re.search(moves_pattern, chunk_text, re.IGNORECASE):
                print(f"  ‚ôü Moves '1.e4 c5' detected in text")
        print("=== END POSITION DEBUG ===\n")

        # Step 6: Synthesize coherent answer using 3-stage pipeline
        print(f"‚è±  Starting 3-stage synthesis pipeline...")
        synthesis_start = time.time()

        # Call the new 3-stage synthesis function
        synthesized_answer = synthesize_answer(
            OPENAI_CLIENT,
            query_text,
            [r['text'] for r in results[:8]]  # Pass top 8 text chunks
        )

        t6 = time.time()
        print(f"‚è±  3-stage synthesis complete: {t6-synthesis_start:.2f}s")

        # Step 6.5: Extract and parse diagram markers from synthesized text
        print(f"\n‚è±  Extracting diagram markers...")
        diagram_start = time.time()

        diagram_positions = extract_diagram_markers(synthesized_answer)
        synthesized_answer = replace_markers_with_ids(synthesized_answer, diagram_positions)

        diagram_time = time.time() - diagram_start
        print(f"‚è±  Diagram extraction complete: {diagram_time:.2f}s")
        print(f"üìã Extracted {len(diagram_positions)} diagram positions from synthesis")

        total = time.time() - start
        print(f"üéØ TOTAL: {total:.2f}s")
        print("=" * 80)

        # DEBUG: Check what's actually being sent to frontend
        print("\n" + "="*60)
        print("FINAL RESPONSE TO FRONTEND - POSITION DEBUG")
        print("="*60)

        for i, source in enumerate(results[:5]):
            print(f"\nSource {i+1}:")
            print(f"  Book: {source.get('book_name', 'unknown')[:50]}")
            print(f"  Has 'positions' key: {'positions' in source}")
            print(f"  Has 'has_positions' key: {'has_positions' in source}")

            if 'positions' in source:
                positions = source['positions']
                print(f"  positions value: {positions}")
                print(f"  positions type: {type(positions)}")
                print(f"  positions length: {len(positions) if positions else 0}")

                if positions:
                    print(f"  First position: {positions[0]}")

            if 'has_positions' in source:
                print(f"  has_positions: {source['has_positions']}")

        print("\n" + "="*60)
        print("END POSITION DEBUG")
        print("="*60 + "\n")

        # Collect positions from top sources for answer section
        synthesized_positions = []
        for result in results[:5]:
            if result.get('has_positions') and result.get('positions'):
                for pos in result['positions']:
                    # Avoid duplicates (same FEN)
                    if not any(p['fen'] == pos['fen'] for p in synthesized_positions):
                        synthesized_positions.append(pos)
                        if len(synthesized_positions) >= 2:  # Max 2 boards in answer
                            break
            if len(synthesized_positions) >= 2:
                break

        print(f"üìã Collected {len(synthesized_positions)} positions for answer section")

        # Prepare response
        response_data = {
            'success': True,
            'query': query_text,
            'answer': synthesized_answer,
            'positions': synthesized_positions,  # Positions extracted from source chunks
            'diagram_positions': diagram_positions,  # NEW: Positions from [DIAGRAM: ...] markers in synthesis
            'sources': results[:5],
            'results': results,  # Keep for backwards compatibility
            'timing': {
                'embedding': round(t2 - t1, 2),
                'search': round(t3 - t2, 2),
                'reranking': round(t4 - t3, 2),
                'formatting': round(t5 - t4, 2),
                'synthesis': round(t6 - synthesis_start, 2),
                'diagrams': round(diagram_time, 2),
                'total': round(total, 2)
            }
        }

        # DEBUG: Log final response structure
        print("\n" + "="*80)
        print("=== FINAL RESPONSE STRUCTURE ===")
        print(f"Response keys: {list(response_data.keys())}")
        print(f"Has 'positions' key: {'positions' in response_data}")
        if 'positions' in response_data:
            print(f"Number of positions: {len(response_data['positions'])}")
            if len(response_data['positions']) > 0:
                print(f"First position keys: {list(response_data['positions'][0].keys())}")
                print(f"First position FEN: {response_data['positions'][0].get('fen', 'N/A')}")
                print(f"First position has SVG: {bool(response_data['positions'][0].get('svg'))}")
        print("="*80 + "\n")

        return jsonify(response_data)

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"ERROR IN /query ENDPOINT:")
        print(error_details)
        return jsonify({'error': str(e), 'details': error_details}), 500


@app.route('/fen_to_lichess', methods=['POST'])
def fen_to_lichess():
    """Convert FEN to Lichess URL."""
    try:
        data = request.get_json()
        fen = data.get('fen', '')

        if not fen:
            return jsonify({'error': 'FEN cannot be empty'}), 400

        url = create_lichess_url(fen)
        return jsonify({'url': url})

    except Exception as e:
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    # Check for API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ùå Error: OPENAI_API_KEY environment variable not set!")
        print("   Set it with: export OPENAI_API_KEY='your-key-here'")
        exit(1)

    print("=" * 80)
    print("SYSTEM A WEB UI")
    print("=" * 80)
    print(f"Corpus: 357,957 chunks from 1,052 books")
    print(f"Starting server at http://127.0.0.1:5001")
    print("=" * 80)

    app.run(debug=False, host='0.0.0.0', port=5001)
